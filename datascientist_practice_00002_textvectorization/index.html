<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>DSP_00002_TextVectorization | Fa7riaBlog</title>
<meta name="description" content="为学日益，为道日损。损之又损，以至于无为，无为而无不为">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="shortcut icon" href="https://fa7ria.GitHub.io/favicon.ico?v=1629890271902">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://unpkg.com/papercss@1.6.1/dist/paper.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://fa7ria.GitHub.io/styles/main.css">


<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />


  </head>
  <body>
  
    <nav class="navbar border fixed split-nav">
  <div class="nav-brand">
    <h3><a href="https://fa7ria.GitHub.io">Fa7riaBlog</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
      <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
        
          <li>
            
              <a href="/" class="menu">
                首页
              </a>
            
          </li>
        
          <li>
            
              <a href="/archives" class="menu">
                归档
              </a>
            
          </li>
        
          <li>
            
              <a href="/tags" class="menu">
                标签
              </a>
            
          </li>
        
          <li>
            
              <a href="/post/about" class="menu">
                关于
              </a>
            
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div id="top" class="row site">
      <div class="sm-12 md-8 col">
        <div class="paper">
          <article class="article">
            <h1>DSP_00002_TextVectorization</h1>
            <p class="article-meta">
              2021-05-09
              
                <a href="https://fa7ria.GitHub.io/HwiRtp_iI/" class="badge success">
                  DataScientist_Practice
                </a>
              
            </p>
            
            <div class="post-content">
              <blockquote>
<p>如果类别小于10，通常采用独热编码方式。如果类别大于50，通常使用嵌入编码。在10到50中间你需要实践比对一番，那种方式更好。</p>
</blockquote>
<p>我们将文本文档转成数值型feature向量的过程称为<strong>向量化（vectorization）</strong>。这种特定的策略（tokenization/counting/normalization）被称为词袋（Bag of Words）或者”Bag of n-grams”表示。通过单词出现率文档描述的文档会完全忽略文档中单词的相对位置。</p>
<h5 id="词嵌入的作用对于原始数据一串符号不能直接传给算法必须将它们表示成使用固定size的数值型的feature向量而非变长的原始文档">词嵌入的作用:对于原始数据，一串符号不能直接传给算法，必须将它们表示成使用固定size的数值型的feature向量，而非变长的原始文档。</h5>
<p>1、使用独热编码分类特征</p>
<p>2、使用嵌入编码分类特征</p>
<p><strong>Feature extraction与Feature Selection是完全不同的：前者将专有数据（文本或图片）转换成机器学习中可用的数值型特征；后者则是选取特征上的技术</strong></p>
<p>为了这个表示，sklearn提供了相应的工具类来从文本内容中抽取数值型feature，对应的功能名为：</p>
<ul>
<li><strong>tokenizing</strong>：对strings进行token化，给每个可能的token一个integer id，例如：使用空格或逗豆做作token分割符。</li>
<li><strong>counting</strong>: 统计在每个文档中token的出现次数。</li>
<li><strong>normalizing和weighting</strong>：对在大多数样本/文档中出现的重要的token进行归一化和降权。</li>
</ul>
<p>在这种scheme下，features和samples的定义如下:</p>
<ul>
<li>每个独立的token出现率（不管是否归一化），都被当成一个feature。</li>
<li>对于一个给定的文档，所有的token频率的向量都被认为是一个<strong>多元样本（multivariate sample）</strong>。</li>
</ul>
<p>一个文档型语料可以被表示成一个矩阵：每个文档一行，每个在语料中出现的token一列（word）。</p>
<p>深度学习模型不会接收原始文本作为输入，它只能处理数值张量。文本向量化（vectorize）是指将文本转换为数值张量的过程。主要有三种实现方式：1.单词级；2字符级；3.提取单词或者字符的n-gram(多个连续的集合）<br>
将文本分解成的单元（单词、字符或 n-gram）叫作标记（token），将文本分解成标记的过程叫作分词（tokenization）。所有文本向量化过程都是应用某种分词方案，然后将数值向量与生成的标记相关联。这些向量组合成序列张量，被输入到深度神经网络中。将向量与标记相关联的主要方法有两种：做 one-hot 编码（one-hot encoding）与标记嵌入［token embedding，通常只用于单词，叫作词嵌入（word embedding）］。</p>
<h5 id="embedding-层">Embedding 层</h5>
<p><u>将 Embedding 层理解为一个字典，能将单词索引映射为密集向量。</u>它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding 层实际上是一种字典查找。</p>
<pre><code class="language-python">#  标记的个数（词汇表大小，这里是 1000，即最大单词索引 +1）和嵌入的维度（这里是 64,有64个特征组成一个单词）
from keras.layers improt Embedding
embedding_layer = Embedding(1000,64)

</code></pre>
<h5 id="embedding-层的输入和输出">Embedding 层的输入和输出：</h5>
<p>输入：一个二维张量，形状为（samples，sequential_length）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数，每个单词对应一个数字，一共sequential_length个单词。<br>
一批数据中的所有序列必须具有相同的长度（因为需要将它们打包成一个张量），短的补0，长的截断<br>
输出：一个三维张量，形状为（samples，sequential_length，dimensionality）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数<br>
dimensionality：代表通道数，同一samples，同一sequential_length上的所有通道上的值组成的向量表示一个单词，如（0，0，：）代表一个单词。</p>
<h5 id="embedding-层的学习和目的">Embedding 层的学习和目的：</h5>
<p>将一个 Embedding 层实例化时，它的权重（即标记向量的内部字典）最开始是随机的，与其他层一样。在训练过程中，利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以利用。一旦训练完成，嵌入空间将会展示大量结构，这种结构专门针对训练模型所要解决的问题。</p>
<h5 id="embedding-层的实践">Embedding 层的实践：</h5>
<p>场景：IMDB 电影评论情感预测任务<br>
方法：将电影评论限制为前 10 000 个最常见的单词（第一次处理这个数据集时就是这么做的），然后将评论长度限制为只有 20 个单词。对于这 10 000 个单词，网络将对每个词都学习一个 8维嵌入，将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量），然后将这个张量展平为二维，最后在上面训练一个 Dense 层用于分类</p>
<p>keras 版本：</p>
<pre><code class="language-python">from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding
 
&quot;&quot;&quot;1.获取数据作为embedding层的输入：将整数列表转换成形状为（samples,maxlen）的二维整数张量&quot;&quot;&quot;
max_features =10000 #前一万个单词组成词汇表，也就是ont-hot里面特征数
maxlen=20#输入的单词序列长度要相同
 
&quot;&quot;&quot;将数据加载为整数列表&quot;&quot;&quot;
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
&quot;&quot;&quot;输入前要截成相同长度&quot;&quot;&quot;
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 
&quot;&quot;&quot;2.使用Embedding 层和分类器&quot;&quot;&quot;
model=Sequential()
model.add(Embedding(10000,8,input_length=maxlen))
&quot;&quot;&quot;将三维的嵌入张量展平成形状为 (samples, maxlen * 8) 的二维张量：
一个sample的所有(maxlen个)单词所有（8个）特征,这就是为什么要截取一样的长度&quot;&quot;&quot;
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])
model.summary()#输出网络结构
#训练模型：自己切分来训练
history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)
</code></pre>
<p>tensorflow 版本：</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

assert tf.__version__ &gt;= &quot;2.0&quot;

from pathlib import Path
from collections import Counter

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
path = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/datasets/aclImdb'



# glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]

# 初始化str为路径 否则报错：AttributeError: 'str' object has no attribute 'glob'
path = Path(path)

train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# TextLineDataset读取csv拆分为neg和pos标签
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)


batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# 输入前要截取成相同程度的文本pad的作用也是保持每个token的单位长度
def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)


# 统计在每个文档中token的出现次数函数
def get_vocabulary(data_sample, max_size=1000):
    preprocessed_reviews = preprocess(data_sample).numpy()
    counter = Counter()
    for words in preprocessed_reviews:
        for word in words:
            if word != b&quot;&lt;pad&gt;&quot;:
                counter[word] += 1
    return [b&quot;&lt;pad&gt;&quot;] + [word for word, count in counter.most_common(max_size)]


get_vocabulary(X_example)


# 1、preprocess 生成 token
# 2、get_vocabulary 统计频率
# 将token统计频率后的形成table与indx接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量
class TextVectorization(keras.layers.Layer):
    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.max_vocabulary_size = max_vocabulary_size
        self.n_oov_buckets = n_oov_buckets

    def adapt(self, data_sample):
        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)
        words = tf.constant(self.vocab)
        word_ids = tf.range(len(self.vocab), dtype=tf.int64)
        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)

    def call(self, inputs):
        preprocessed_inputs = preprocess(inputs)
        return self.table.lookup(preprocessed_inputs)


text_vectorization = TextVectorization()

text_vectorization.adapt(X_example)
text_vectorization(X_example)

max_vocabulary_size = 1000
n_oov_buckets = 100

sample_review_batches = train_set.map(lambda review, label: review)
sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),
                                axis=0)

text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,
                                       input_shape=[])
text_vectorization.adapt(sample_reviews)
text_vectorization(X_example)

simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])
tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)

print('text_vectorization.vocab: ', text_vectorization.vocab[:10])


class BagOfWords(keras.layers.Layer):
    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.n_tokens = n_tokens

    def call(self, inputs):
        one_hot = tf.one_hot(inputs, self.n_tokens)
        return tf.reduce_sum(one_hot, axis=1)[:, 1:]


bag_of_words = BagOfWords(n_tokens=4)
print('bag_of_words(simple_example): ', bag_of_words(simple_example))

n_tokens = max_vocabulary_size + n_oov_buckets + 1  # add 1 for &lt;pad&gt;
bag_of_words = BagOfWords(n_tokens)

model = keras.models.Sequential([
    text_vectorization,
    bag_of_words,
    keras.layers.Dense(100, activation=&quot;relu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;),
])
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])
model.fit(train_set, epochs=5, validation_data=valid_set)


</code></pre>

            </div>
          </article>
        </div>
        <div class="paper" data-aos="fade-in">
          
            <div class="next-post">
              <div class="next">
                下一篇
              </div>
              <a href="https://fa7ria.GitHub.io/datascientist_practice_00001/">
                <h3 class="post-title">
                  DSP_00001
                </h3>
              </a>
            </div>
          
        </div>
        
      </div>

      <div class="sm-12 md-4 col sidebar">
  <div class="paper info-container">
    <img src="https://fa7ria.GitHub.io/images/avatar.png?v=1629890271902" class="no-responsive avatar">
    <div class="text-muted">为学日益，为道日损。损之又损，以至于无为，无为而无不为</div>
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      最新文章
    </div>
    <div class="row">
      <ul>
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/dsp_00008_leetcode006/">DSP_00008_Leetcode006</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/dsp_00007_entity_embedding/">DSP_00007_entity_embedding</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_practice_00006_parameters_cv/">DSP_00006_parameters_cv</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/dsp_00005_stacking_house_price/">DSP_00005_Stacking_house_price</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/dsp_00003_tfrecords/">DSP_00003_TFRecords</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/">DSP_00002_TextVectorization</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_practice_00001/">DSP_00001</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">DSK_999</a>
            </li>
          
        
      </ul>
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      标签列表
    </div>
    <div class="row">
      
        <a href="https://fa7ria.GitHub.io/HwiRtp_iI/" class="badge ">
          DataScientist_Practice
        </a>
      
        <a href="https://fa7ria.GitHub.io/ZsI6TLsBB/" class="badge success">
          DataScientist_Knowledge
        </a>
      
    </div>
  </div>
  <div class="paper">
    Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://fa7ria.GitHub.io/atom.xml" target="_blank">RSS</a>
  </div>
</div>


    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

</script>




  </body>
</html>
