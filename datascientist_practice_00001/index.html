<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>DataScientist_Practice_00001 | Fa7riaBlog</title>
<meta name="description" content="为学日益，为道日损。损之又损，以至于无为，无为而无不为">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="shortcut icon" href="https://fa7ria.GitHub.io/favicon.ico?v=1620473589437">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://unpkg.com/papercss@1.6.1/dist/paper.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://fa7ria.GitHub.io/styles/main.css">


<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />


  </head>
  <body>
  
    <nav class="navbar border fixed split-nav">
  <div class="nav-brand">
    <h3><a href="https://fa7ria.GitHub.io">Fa7riaBlog</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
      <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
        
          <li>
            
              <a href="/" class="menu">
                首页
              </a>
            
          </li>
        
          <li>
            
              <a href="/archives" class="menu">
                归档
              </a>
            
          </li>
        
          <li>
            
              <a href="/tags" class="menu">
                标签
              </a>
            
          </li>
        
          <li>
            
              <a href="/post/about" class="menu">
                关于
              </a>
            
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div id="top" class="row site">
      <div class="sm-12 md-8 col">
        <div class="paper">
          <article class="article">
            <h1>DataScientist_Practice_00001</h1>
            <p class="article-meta">
              2021-05-08
              
            </p>
            
            <div class="post-content">
              <blockquote>
<p>Task：对数据集进行拆分，创建一个tf.data.Dataset来加载并进行预处理，然后创建和训练一个包含Embedding层的二进制分类模型</p>
</blockquote>
<p>1.下载大型电影评论数据集；</p>
<p>2.将测试集拆分为一个验证集和一个测试集</p>
<p>3.创建一个二进制分类模型，使用TextVectorization层对每个评论进行预处理。如果TextVectorization层尚不可用，请自定义预处理层。</p>
<pre><code># TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

from IPython import get_ipython

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# ## 10.
# _Exercise: In this exercise you will download a dataset, split it,
# create a `tf.data.Dataset` to load it and preprocess it efficiently,
# then build and train a binary classification model containing an `Embedding` layer._
#
# ### a.
# _Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb),
# which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/).
# The data is organized in two directories, `train` and `test`,
# each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews.
# Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words),
# but we will ignore them in this exercise._




from pathlib import Path

# 使用keras.utils.get_file下载imdb数据集，cache_dir指定自己的文件目录；
DOWNLOAD_ROOT = &quot;http://ai.stanford.edu/~amaas/data/sentiment/&quot;
FILENAME = &quot;aclImdb_v1.tar.gz&quot;
filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True,cache_dir=input_dir)
path = Path(filepath).parent / &quot;aclImdb&quot;
print(path)
# C:\Users\ericf\PycharmProjects\data_scientenist\data\raw\datasets\aclImdb

# 打印数据集的层级关系和文件名
#  os.walk根目录下的每一个文件夹(包含它自己), 产生3-元组 (dirpath, dirnames, filenames)【文件夹路径, 文件夹名字, 文件名】
#  name, subdirs, files=【文件夹路径, 文件夹名字, 文件名】
#  indent 根据文件夹目录记录缩进
for name, subdirs, files in os.walk(path):
    indent = len(Path(name).parts) - len(path.parts)
    print(&quot;    &quot; * indent + Path(name).parts[-1] + os.sep)
    for index, filename in enumerate(sorted(files)):
        if index == 3:
            print(&quot;    &quot; * (indent + 1) + &quot;...&quot;)
            break
        print(&quot;    &quot; * (indent + 1) + filename)

#glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]


train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

print(len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg))

# ### b.
# _Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._
# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# ### c.
# _Exercise: Use tf.data to create an efficient dataset for each set._

# Since the dataset fits in memory,
# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:

# 对于影评进行二分类标签处理
def imdb_dataset(filepaths_positive, filepaths_negative):
    reviews = []
    labels = []
    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):
        for filepath in filepaths:
            with open(filepath,encoding='utf-8') as review_file:
                reviews.append(review_file.read())
            labels.append(label)
    return tf.data.Dataset.from_tensor_slices(
        (tf.constant(reviews), tf.constant(labels)))



for X, y in imdb_dataset(train_pos, train_neg).take(3):
    print(X)
    print(y)
    print()

#get_ipython().run_line_magic('timeit', '-r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass')


# It takes about 17 seconds to load the dataset and go through it 10 times.

# But let's pretend the dataset does not fit in memory, just to make things more interesting.
# Luckily, each review fits on just one line (they use `&lt;br /&gt;` to indicate line breaks),
# so we can read the reviews using a `TextLineDataset`.
# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).
# For very large datasets, it would make sense to use a tool like Apache Beam for that.


# 1.tf.data.TextLineDataset 读取数据集，自动创建dataset
# 2.读取之后使用map根据pos和neg进行打标签操作
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)




# Now it takes about 33 seconds to go through the dataset 10 times. That's much slower,
# essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.
# If you add `.cache()` just before `.repeat(10)`,
# you will see that this implementation will be about as fast as the previous one.



batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# ### d.
# _Exercise: Create a binary classification model,
# using a `TextVectorization` layer to preprocess each review.
# If the `TextVectorization` layer is not yet available (or if you like a challenge),
# try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package,
# for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces,
# and `split()` to split words on spaces.
# You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._

# Let's first write a function to preprocess the reviews, cropping them to 300 characters,
# converting them to lower case, then replacing `&lt;br /&gt;` and all non-letter characters to spaces,
# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:




def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)
</code></pre>

            </div>
          </article>
        </div>
        <div class="paper" data-aos="fade-in">
          
            <div class="next-post">
              <div class="next">
                下一篇
              </div>
              <a href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">
                <h3 class="post-title">
                  DataScientist_Kownledge_999
                </h3>
              </a>
            </div>
          
        </div>
        
      </div>

      <div class="sm-12 md-4 col sidebar">
  <div class="paper info-container">
    <img src="https://fa7ria.GitHub.io/images/avatar.png?v=1620473589437" class="no-responsive avatar">
    <div class="text-muted">为学日益，为道日损。损之又损，以至于无为，无为而无不为</div>
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      最新文章
    </div>
    <div class="row">
      <ul>
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_practice_00001/">DataScientist_Practice_00001</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">DataScientist_Kownledge_999</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/about/">关于</a>
            </li>
          
        
          
            <li>
              <a href="https://fa7ria.GitHub.io/hello-gridea/">Hello Gridea</a>
            </li>
          
        
      </ul>
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      标签列表
    </div>
    <div class="row">
      
        <a href="https://fa7ria.GitHub.io/ZsI6TLsBB/" class="badge secondary">
          DataScientist_Knowledge
        </a>
      
        <a href="https://fa7ria.GitHub.io/6J4KjEj3k/" class="badge warning">
          Gridea
        </a>
      
    </div>
  </div>
  <div class="paper">
    Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://fa7ria.GitHub.io/atom.xml" target="_blank">RSS</a>
  </div>
</div>


    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

</script>




  </body>
</html>
