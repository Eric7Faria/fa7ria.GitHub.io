<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://fa7ria.GitHub.io</id>
    <title>Fa7riaBlog</title>
    <updated>2021-08-30T03:42:54.193Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://fa7ria.GitHub.io"/>
    <link rel="self" href="https://fa7ria.GitHub.io/atom.xml"/>
    <subtitle>为学日益，为道日损。损之又损，以至于无为，无为而无不为</subtitle>
    <logo>https://fa7ria.GitHub.io/images/avatar.png</logo>
    <icon>https://fa7ria.GitHub.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Fa7riaBlog</rights>
    <entry>
        <title type="html"><![CDATA[DSP_00009_LeetCode005]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00009_leetcode006/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00009_leetcode006/">
        </link>
        <updated>2021-08-26T13:59:06.000Z</updated>
        <content type="html"><![CDATA[<p>给你一个字符串 <code>s</code>，找到 <code>s</code> 中最长的回文子串。</p>
<blockquote>
<pre><code class="language-python">输入：s = &quot;babad&quot;
输出：&quot;bab&quot;
解释：&quot;aba&quot; 同样是符合题意的答案。
</code></pre>
</blockquote>
<p>（一）中心扩展法，最直观的方法,可读性最高的解法<br>
（1）以每个位置为中心，向两边扩散。<br>
分为palindrome_str长度为奇数和偶数2种情况<br>
（2）传递下去的条件是s[L]==s[R]</p>
<pre><code class="language-python">def longestPalindrome( s):
    n = len(s)
    if n &lt; 2:
        return s

    ##############中心扩展法，最直观的方法
    def center_spread(L, R):
        while 0 &lt;= L and R &lt; n and s[L] == s[R]:
            L -= 1
            R += 1
        return s[L + 1: R]

    res = s[0]
    max_len = 1

    for i in range(n):
        odd_str = center_spread(i, i)
        even_str = center_spread(i, i + 1)

        if len(odd_str) &gt; len(even_str):  # 若长度为奇数的长
            if len(odd_str) &gt; max_len:
                max_len = len(odd_str)
                res = odd_str
        else:  # 若长度为偶数的长
            if len(even_str) &gt; max_len:
                max_len = len(even_str)
                res = even_str

    return res






s = &quot;babad&quot;
rslt = longestPalindrome(s)

print(rslt)
</code></pre>
<p>（二）manacher马拉车算法<br>
（1）<br>
arm_len数组。<br>
与kmp有异曲同工之妙kmp需要预计算，求一个next数组<br>
manacher和kmp的辅助数组，保存的本质都是各个位置的长度！！</p>
<p>预处理：每个字母的左右侧都插入了‘#’----只考虑回文串长度为奇数就ok了</p>
<p>（2）<br>
if：maxR - i &gt;= 0<br>
#i在maxR内部的时候，分为完全在内部(&gt;0)或者压界(=0),在压界的情况需要扩展调试</p>
<p>else:<br>
#i在maxR外部的时候，只能暴力扩展</p>
<p>（3）每次更新<br>
center:中心对称点<br>
maxR：当前可达的最右端</p>
<p>（4）每次更新<br>
最长回文串在t串中的起点res_start和尾点res_end</p>
<p>(5)<br>
返回结果时，注意剔掉‘#’</p>
<pre><code class="language-python">def longestPalindrome(self, s: str) -&gt; str:
    t = '#' + ('#'.join(list(s))) + '#'
    n = len(t)

    arm_len = []        ##臂长数组 kmp算法的next数组也是存的长度 
    res_start, res_end = 0, 0   ##最长回文串，在t串中的起点和尾点
    maxR = -1           #当前可达的最右端
    center = -1         #当前的中心
    for i in range(n):  ####遍历每个位置，从左至右
        cur_arm_len = 0     #最前的臂长
        if maxR - i &gt;= 0:       #i在maxR内部的时候，分为完全在内部(&gt;0)或者压界(=0),在压界的情况需要扩展调试
            mirror_i = 2 * center - i   #i关于center左侧的镜像点
            cur_min_len = min(arm_len[mirror_i], maxR - i)
            ###### 还要 中心扩散 尝试一下
            cur_arm_len = self.center_spread(t, i - cur_min_len, i + cur_min_len)
        else:               #i在maxR外部的时候，只能暴力扩展
            cur_arm_len = self.center_spread(t, i, i)
        ####### 存入arm_len数组
        arm_len.append(cur_arm_len)
        ####### 更新maxR，当前可以到达的最右端,更新center
        if i + cur_arm_len &gt; maxR:
            maxR = i + cur_arm_len
            center = i
        ####### 更新结果，最长回文串，在t中的起点和尾点的index
        if (2 * cur_arm_len + 1) &gt; (res_end - res_start):
            res_start = i - cur_arm_len
            res_end = i + cur_arm_len

    res = &quot;&quot;
    for i in range(res_start, res_end + 1):
        if t[i] != '#':
            res += t[i]
    return res
####### 在t串中 中心扩散 ，返回在s串中的回文串长度
def center_spread(self, t: str, L: int, R: int) -&gt; int:
    n = len(t)
    while 0&lt;=L and R&lt;n and t[L]==t[R]:
        L -= 1
        R += 1
    return (R-L-2) // 2
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00008_Leetcode006]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00008_leetcode006/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00008_leetcode006/">
        </link>
        <updated>2021-08-25T10:54:31.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>将一个给定字符串 s 根据给定的行数 numRows ，以从上往下、从左到右进行 Z 字形排列。</p>
<p>比如输入字符串为 &quot;PAYPALISHIRING&quot; 行数为 3 时，排列如下：</p>
<pre><code>P   A   H   N
A P L S I I G
Y   I   R
</code></pre>
</blockquote>
<p>关键词：行索引递增递减<br>
base case：len(s) &lt;= 2 or numRows==1</p>
<p>程序：<br>
创建长度为numRows的空列表，每个元素依次储存z字形的每一行<br>
可能性条件：s中的每个字母的行索引先按1递增，直到numRows时按1递减，减到0时再次按1递增，如此反复</p>
<pre><code class="language-python">def convert(s, numRows):
    if len(s) &lt;= 2 or numRows==1:
        return s

    res = ['' for _ in range(numRows)]

    i = 0
    direct = -1
    for c in s:
        res[i] = res[i] + c
        if i == 0 or i == numRows - 1:
            direct = -direct
        i = i + direct
    print(res)
    return ''.join(res)

s, numRows = &quot;PAYPALISHIRING&quot;, 3
rslt = convert(s,numRows)
print(rslt)

s2, numRows2 = &quot;adsdccccs&quot;, 4
rslt2 = convert(s2,numRows2)

print(rslt2)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00007_entity_embedding]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00007_entity_embedding/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00007_entity_embedding/">
        </link>
        <updated>2021-05-25T08:15:51.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>机器学习中处理分类变量的三种常见方法</p>
<ol>
<li>one-hot encoding\label encoding</li>
<li>histgram 映射</li>
<li>Entity Embeddings（实体嵌入）</li>
</ol>
</blockquote>
<h2 id="histgram-映射">histgram 映射</h2>
<p>比如我们根据人们的一些身体特征来预测这个人会不会得糖尿病,其中有一个类别特征为男或者女，这时候我们如何对这个特征进行映射处理呢。按照histgram 的处理方式，会按照预测的类别的占比来标注属性。我们可以男：60/(60+40) = 0.6；女：50/(50+50) = 0.5。作为各自的特征。</p>
<table>
<thead>
<tr>
<th style="text-align:center">性别\是否得病</th>
<th style="text-align:center">有糖尿病</th>
<th style="text-align:center">无糖尿病</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">男</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
</tr>
<tr>
<td style="text-align:center">女</td>
<td style="text-align:center">50</td>
<td style="text-align:center">50</td>
</tr>
</tbody>
</table>
<h3 id="entity-embeddings">Entity Embeddings</h3>
<p>Embedding的起源和火爆都是在NLP中的，经典的word2vec都是在做word embedding这件事情，而真正首先在结构数据探索embedding的是在kaggle上的《Rossmann Store Sales》中的rank 3的解决方案:结构非常简单，就是embedding层后面接上了两个全连接层，代码用keras写的，构建模型的代码量也非常少，用的keras的sequence model。</p>
<p>文章有几点分析比较值得关注的地方。</p>
<p>1、店铺所在地的嵌入向量在用TSNE投影到两维空间后和地图位置有着极大的相似性。<br>
2、使用嵌入后的向量可以提高其他算法（KNN、随机森林、gdbt）的准确性。<br>
3、作者探索了embedding和度量空间之间的联系，试图从数学层面深入探讨embedding的作用。</p>
<pre><code class="language-python">## 《Rossmann Store Sales》中的rank 3的解决方案 keras source code
def build_embedding_network():
    inputs = []
    embeddings = []
    for i in range(len(embed_cols)):
        cate_input = Input(shape=(1,))
        input_dim = len(col_vals_dict[embed_cols[i]])
        if input_dim &gt; 1000:
            output_dim = 50
        else:
            output_dim = (len(col_vals_dict[embed_cols[i]]) // 2) + 1
 
        embedding = Embedding(input_dim, output_dim, input_length=1)(cate_input)
        embedding = Reshape(target_shape=(output_dim,))(embedding)
        inputs.append(cate_input)
        embeddings.append(embedding)
 
    input_numeric = Input(shape=(4,))
    embedding_numeric = Dense(5)(input_numeric)
    inputs.append(input_numeric)
    embeddings.append(embedding_numeric)
 
    x = Concatenate()(embeddings)
    x = Dense(300, activation='relu')(x)
    x = Dropout(.35)(x)
    x = Dense(100, activation='relu')(x)
    x = Dropout(.15)(x)
    output = Dense(1, activation='sigmoid')(x)
 
    model = Model(inputs, output)
 
    model.compile(loss='binary_crossentropy', optimizer='rmsprop')
 
    return model
    
    
    
</code></pre>
<pre><code class="language-python"># -*- coding: utf-8 -*-


'''
描述：DataScientist_Practice_00007
    实体嵌入的代码练习
作者：liuyi
程序开发环境：win 64位
Python版本：64位 3.8.3（使用Anaconda安装）
python IDE：PyCharm 2020.1专业版
依赖库：tensorflow,keras,IPython,numpy,os,GraphViz
程序输入：具体查看各模块
程序输出：具体查看各模块

'''
####################################################################


import numpy as np

import tensorflow as tf
import random as rn

# random seeds for stochastic parts of neural network
np.random.seed(10)


from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate, Reshape, Dropout, Embedding

input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'

# ===================================================================================================

# 记录类别特征embedding后的维度字典。key为类别特征索引，value为embedding后的维度
cate_embedding_dimension = {'0': 3, '1': 2}

# 需要手工设置每个特征的转换封装
def build_embedding_network():
    # 以网络结构embeddding层在前，dense层在后。即训练集的X必须以分类特征在前，连续特征在后。
    inputs = []
    embeddings = []

    input_cate_feature_1 = Input(shape=(1,))
    embedding = Embedding(10, 3, input_length=1)(input_cate_feature_1)
    # embedding后是10*1*3，为了后续计算方便，因此使用Reshape转为10*3
    embedding = Reshape(target_shape=(3,))(embedding)
    inputs.append(input_cate_feature_1)
    embeddings.append(embedding)

    input_cate_feature_2 = Input(shape=(1,))
    embedding = Embedding(4, 2, input_length=1)(input_cate_feature_2)
    embedding = Reshape(target_shape=(2,))(embedding)
    inputs.append(input_cate_feature_2)
    embeddings.append(embedding)

    input_numeric = Input(shape=(1,))
    embedding_numeric = Dense(16)(input_numeric)
    inputs.append(input_numeric)
    embeddings.append(embedding_numeric)

    x = Concatenate()(embeddings)

    x = Dense(10, activation='relu')(x)
    x = Dropout(.15)(x)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, output)

    model.compile(loss='binary_crossentropy', optimizer='adam')
    model.summary()

    return model


# ===================================================================================================
# 程序入口
# ===================================================================================================
'''
输入数据是32*3，32个样本，2个类别特征，1个连续特征。
对类别特征做entity embedding，第一个类别特征的可能值是0到9之间（10个），第二个类别特征的可能值是0到3之间（4个）。
对这2个特征做one-hot的话，应该为32*14，
对第一个类别特征做embedding使其为3维，对第二个类别特征做embedding使其为2维。3维和2维的设定是根据实验效果和交叉验证设定。
对连续特征不做处理。
这样理想输出的结果就应该是32*6，其中，类别特征维度为5，连续特征维度为1。
'''
# ===================================================================================================
# 构造训练数据
# ===================================================================================================

sample_num = 32  # 样本数为32
cate_feature_num = 2  # 类别特征为2
contious_feature_num = 1  # 连续特征为1

# 保证了训练集的复现
rng = np.random.RandomState(17)
cate_feature_1 = rng.randint(10, size=(32, 1))
cate_feature_2 = rng.randint(4, size=(32, 1))
contious_feature = rng.rand(32, 1)
X = []
X.append(cate_feature_1)
X.append(cate_feature_2)
X.append(contious_feature)
# 二分类
Y = np.random.randint(2, size=(32, 1))
# print('x:', X)

# ===================================================================================================
# 训练和预测
# ===================================================================================================
# train
NN = build_embedding_network()
NN.fit(X, Y, epochs=3, batch_size=4, verbose=0)
# predict
y_preds = NN.predict(X)[:, 0]

# 画出模型，需要GraphViz包。
# from tensorflow.keras.utils import plot_model
# output_file = output_dir + 'embedding_nn.png'
# plot_model(NN, to_file=output_file)

# ===================================================================================================
# 读embedding层的输出结果
# ===================================================================================================

model = NN  # 创建原始模型
for i in range(cate_feature_num):
    # 由NN.png图可知，如果把类别特征放前，连续特征放后，cate_feature_num+i就是所有embedding层
    layer_name = NN.get_config()['layers'][cate_feature_num + i]['name']

    intermediate_layer_model = Model(inputs=NN.input,
                                     outputs=model.get_layer(layer_name).output)

    # numpy.array
    intermediate_output = intermediate_layer_model.predict(X)

    # print('intermediate_output:',intermediate_output)

    intermediate_output.resize([32, cate_embedding_dimension[str(i)]])

    if i == 0:
        X_embedding_trans = intermediate_output
    else:
        X_embedding_trans = np.hstack((X_embedding_trans, intermediate_output))  # 水平拼接

# 取出原来的连续特征。这里的list我转numpy一直出问题，被迫这么写循环了。
for i in range(contious_feature_num):
    if i == 0:
        X_contious = X[cate_feature_num + i]
    else:
        X_contious = np.hstack((X_contious, X[cate_feature_num + i]))

# ===================================================================================================
# 在类别特征做embedding后的基础上，拼接连续特征，形成最终矩阵，也就是其它学习器的输入
# ===================================================================================================

'''
最终的结果：32*6.其中，类别特征维度为5（前5个），连续特征维度为1（最后1个）
'''
X_trans = np.hstack((X_embedding_trans, X_contious))
print('X_trans:',X_trans)


# ===================================================================================================
# ===================================================================================================
# solution 2:
# ===================================================================================================
# ===================================================================================================

import numpy as np
import tensorflow as tf
import random as rn

# random seeds for stochastic parts of neural network
np.random.seed(7)


from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.layers import Input, Dense, Concatenate, Reshape, Dropout, Embedding
 

 
 
'''
输入数据是32*2，32个样本，2个类别特征，且类别特征的可能值是0到9之间(10个)。
对这2个特征做one-hot的话，应该为32*20，
embedding就是使1个特征原本应该one-hot的10维变为3维(手动设定，也可以是其它)，因为有2个类别特征
这样输出的结果就应该是32*6
'''
model = Sequential()
model.add(Embedding(10, 3, input_length=2))
 
# 构造输入数据
input_array = np.random.randint(10, size=(32, 2))
 
# 搭建模型
model.compile('rmsprop', 'mse')
 
# 得到输出数据 输出格式为32*2*3。我们最终想要的格式为32*6，其实就是把2*3按照行拉成6维，然后就是我们对类别特征进行
# embedding后得到的结果了。
output_array = model.predict(input_array)
 
# 查看权重参数
weight = model.get_weights()

# ===================================================================================================
# ===================================================================================================
# solution 3:
# ===================================================================================================
# ===================================================================================================
import tensorflow as tf 
import pandas as pd
import numpy as np
from pandas import Series,DataFrame
import warnings
warnings.filterwarnings('ignore') 
import os
 
file_name_string=&quot;E:/my_create_resource/deeplearning/data/ml-latest-small/movies.csv&quot;
filename_queue = tf.train.string_input_producer([file_name_string])
reader = tf.TextLineReader()
_,value = reader.read(filename_queue)
&quot;&quot;&quot;
value = [
  &quot;1,ToyStory,Adventure|Animation|Children|Comedy|Fantasy&quot;,
&quot;2,Toytory,Adventure|Animation|Children|Comedy|Fantasy&quot; 
]
&quot;&quot;&quot;
### Warning：获得TAG_SET——因为官方数据readme里面说Children's（实际上是Children），浪费半个小时找格式错误
TAG_SET = [&quot;Action&quot;,&quot;Adventure&quot;,&quot;Animation&quot;,&quot;Children&quot;,&quot;Comedy&quot;,&quot;Crime&quot;,&quot;Documentary&quot;,&quot;Drama&quot;,
&quot;Fantasy&quot;,&quot;Film-Noir&quot;,&quot;Horror&quot;,&quot;Musical&quot;,&quot;Mystery&quot;,&quot;Romance&quot;,&quot;Sci-Fi&quot;,&quot;Thriller&quot;,&quot;War&quot;,&quot;Western&quot;]
 
def sparse_from_csv(csv):
    ids, col2,col3 = tf.decode_csv(csv, [[-1], [&quot;&quot;],[&quot;&quot;]]) 
    #保证样本和标签一一对应 这里的batch_size是超参TODO
    ids_batch, col2_batch,col3_batch = tf.train.batch([ids, col2,col3], name='movies',batch_size=1, capacity=200, num_threads=2)    
    table = tf.contrib.lookup.index_table_from_tensor(
      mapping=TAG_SET, default_value=-1) ## 这里构造了个查找表 ##
    
    split_tags = tf.string_split(col3_batch, &quot;|&quot;)
    
    return ids_batch,col2_batch,tf.SparseTensor(
      indices=split_tags.indices,
      values=table.lookup(split_tags.values), ## 这里给出了不同值通过表查到的index ##
      dense_shape=split_tags.dense_shape)
 
TAG_EMBEDDING_DIM = 3
## 生成TAG_SET这十八个类别的embedding值，之后使用
embedding_params = tf.Variable(tf.truncated_normal([len(TAG_SET), TAG_EMBEDDING_DIM]))
 
ids,names,tags = sparse_from_csv(value)
embedded_tags = tf.nn.embedding_lookup_sparse(embedding_params, sp_ids=tags, sp_weights=None)


</code></pre>
<h2 id="总结">总结</h2>
<p>1、想通过自己的模型和自定义层级进行entity embedding就通过以上的代码操作，读取model.get_layer(layer_name).output；或者使用上面的solution2的方法创建Sequential()的model。</p>
<p>2、单列的单值转换：如果是使用tensorflow的Esitmator来建模，可以直接使用<strong>tf.feature_column.embedding_column</strong>构建特征列**，**直接将数据给它，它就给我们embedding好了。</p>
<p>​	单列的多值转换：就是一条数据会拥有该属性多个值，而非一个，那么需要调用函数tf.feature_column.embedding_lookup_sparse‘；参考代码中solution3</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00006_parameters_cv]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00006_parameters_cv/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00006_parameters_cv/">
        </link>
        <updated>2021-05-24T12:10:47.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>机器学习中四中常见的超参数调优</p>
<p>1、传统手工调参</p>
<p>2、网格搜索【GridSearchCV】</p>
<p>3、随机搜索【RandomizedSearchCV】</p>
<p>4、贝叶斯搜索【skopt.BayesSearchCV】</p>
</blockquote>
<h3 id="贝叶斯搜索">贝叶斯搜索</h3>
<blockquote>
<p><strong>贝叶斯优化</strong>用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程**,直到后验分布基本贴合于真实分布。简单的说，就是**考虑了上一次参数的信息，从而更好的调整当前的参数。</p>
<p>他与常规的网格搜索或者随机搜索的区别是：</p>
<ul>
<li>贝叶斯调参采用高斯过程，<strong>考虑之前的参数信息</strong>，不断地更新先验；网格搜索未考虑之前的参数信息</li>
<li>贝叶斯调参<strong>迭代次数少，速度快</strong>；网格搜索速度慢,参数多时易导致维度爆炸</li>
<li>贝叶斯调参针对非凸问题依然<strong>稳健</strong>；网格搜索针对非凸问题易得到局部最优</li>
</ul>
</blockquote>
<p>贝叶斯优化属于一类优化算法，称为基于序列模型的优化(SMBO)算法。这些算法使用先前对损失<strong>f</strong>的观察结果，以确定下一个(最优)点来抽样<strong>f</strong>。</p>
<p>该算法大致可以概括如下。</p>
<ol>
<li>使用先前评估的点<strong>X</strong>1*:n*，计算损失<strong>f</strong>的后验期望。</li>
<li>在新的点<strong>X</strong>的抽样损失<strong>f</strong>，从而最大化<strong>f</strong>的期望的某些方法。该方法指定<strong>f</strong>域的哪些区域最适于抽样。</li>
</ol>
<p>重复这些步骤，直到满足某些收敛准则。</p>
<pre><code class="language-python"># -*- coding: utf-8 -*-


'''
描述：DataScientist_Practice_00006
    贝叶斯超参数调优，在实践中，想枚举多个预测模型类，和不同的搜索空间和每个类的计算数。
一个在线性支持向量机，核支持向量机和决策树

作者：liuyi
程序开发环境：win 64位
Python版本：64位 3.8.3（使用Anaconda安装）
python IDE：PyCharm 2020.1专业版
依赖库：tensorflow,keras,IPython,numpy,os,skopt
程序输入：具体查看各模块
程序输出：具体查看各模块

'''
####################################################################


import numpy as np
import pandas as pd

import matplotlib.pyplot as plt



#############################################################################

# Minimal example

# ===============

#

# A minimal example of optimizing hyperparameters of SVC (Support Vector machine Classifier) is given below.



from skopt import BayesSearchCV
from sklearn.datasets import load_digits
from sklearn.svm import SVC,SVR
from sklearn.model_selection import train_test_split


# 基本状态查看
def set_summary(df):
    '''
    查看数据集的记录数、维度数、前2条数据、描述性统计和数据类型
    :param df: 数据框
    :return: 无
    '''
    print ('Data Overview')
    print ('Records: {0}\tDimension{1}'.format(df.shape[0], (df.shape[1] - 1)))  # 打印数据集X形状
    print ('-' * 30)
    print (df.head(2))  # 打印前2条数据
    print ('-' * 30)
    print ('Data DESC')
    print (df.describe())  # 打印数据基本描述性信息
    # print ('Data Dtypes')
    # print (df.dtypes)  # 打印数据类型
    print ('-' * 60)

# 缺失值审查
def na_summary(df):
    '''
    查看数据集的缺失数据列、行记录数
    :param df: 数据框
    :return: 无
    '''
    na_cols = df.isnull().any(axis=0)  # 查看每一列是否具有缺失值
    print('{:*^60}'.format('NA Cols:'))
    print(na_cols)  # 查看具有缺失值的列
    print('Total number of NA lines is: {0}'.format(df.isnull().any(axis=1).sum()))  # 查看具有缺失值的行总记录数


# 加载数据集
# X, y = load_digits(n_class=10, return_X_y=True)
# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)


input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'

train_path_file = input_dir + 'sh_600535_2018_2021.xlsx'

raw_data = pd.read_excel(train_path_file)
set_summary(raw_data)
na_summary(raw_data)
X = raw_data.iloc[:,1:-1]
y = raw_data['close']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=7)



# log-uniform: understand as search over p = exp(x) by varying x
opt = BayesSearchCV(
    #SVC(),
    SVR(),
    {
        'C': (1e-6, 1e+6, 'log-uniform'),
        'gamma': (1e-6, 1e+1, 'log-uniform'),
        'degree': (1, 3),  # integer valued parameter
        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter
    },
    n_iter=32,
    cv=3
)

opt.fit(X_train, y_train)
#best parameter combination
print(&quot;best parameter combination: %s&quot; % opt.best_params_)
print(&quot;val. score: %s&quot; % opt.best_score_)
print(&quot;test score: %s&quot; % opt.score(X_test, y_test))




</code></pre>
<h2 id="总结">总结</h2>
<p>确定的先验经验的情况下：在确定参数的最佳组合和计算时间之间探索平衡。</p>
<p>如果超参数空间(超参数个数)非常大，则使用随机搜索找到超参数的潜在组合，然后在该局部使用网格搜索(超参数的潜在组合)选择最优特征。或者再确定范围后使用贝叶斯搜索。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00005_Stacking_house_price]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00005_stacking_house_price/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00005_stacking_house_price/">
        </link>
        <updated>2021-05-23T01:04:50.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<pre><code>Stacking 是什么?
Stacking简单理解就是讲几个简单的模型，一般采用将它们进行K折交叉验证输出预测结果， 然后将每个模型输出的预测结果合并为新的特征，并使用新的模型加以训练。
</code></pre>
</blockquote>
<pre><code class="language-python"># -*- coding: utf-8 -*-


'''
描述：DataScientist_Practice_00005
    hands-on ml charpt13 practice
    Stacking 是什么?
作者：liuyi
程序开发环境：win 64位
Python版本：64位 3.8.3（使用Anaconda安装）
python IDE：PyCharm 2020.1专业版
依赖库：tensorflow,keras,IPython,numpy,os
程序输入：具体查看各模块
程序输出：具体查看各模块

'''
####################################################################





# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# Stacking 是什么?
# Stacking简单理解就是讲几个简单的模型，一般采用将它们进行K折交叉验证输出预测结果，
# 然后将每个模型输出的预测结果合并为新的特征，并使用新的模型加以训练。
#
# ###
# 主要的过程简述如下：
# 1、首先需要几个模型，然后对已有的数据集进行K折交叉验证
# 2、K折交叉验证训练集，对每折的输出结果保存，最后进行合并
# 3、对于测试集T1的得到，有两种方法。注意到刚刚是2折交叉验证，M1相当于训练了2次，
#  所以一种方法是每一次训练M1，可以直接对整个test进行预测，这样2折交叉验证后测试集相当于预测了2次，然后对这两列求平均得到T1。
# 4、是两层循环，第一层循环控制基模型的数目，第二层循环控制的是交叉验证的次数K，对每一个基模型会训练K次，然后拼接得到预测结果P1。
# 5、该图是一个基模型得到P1和T1的过程，采用的是5折交叉验证，所以循环了5次，拼接得到P1，测试集预测了5次，取平均得到T1。
#   而这仅仅只是第二层输入的一列/一个特征，并不是整个训练集。再分析作者的代码也就很清楚了。也就是刚刚提到的两层循环
#import some necessary librairies

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
# ignore annoying warning (from sklearn and seaborn)
warnings.warn = ignore_warn
from sklearn.preprocessing import LabelEncoder
from scipy import stats
from scipy.stats import norm, skew #for some statistics
from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb

# 设置输出的浮点数的格式
pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))

from subprocess import check_output
# check the files available in the directory
# print(check_output([&quot;ls&quot;, &quot;/Users/liudong/Desktop/house_price/train.csv&quot;]).decode(&quot;utf8&quot;))
# 加载数据
train_file = input_dir + 'house_prices/train.csv'
test_file = input_dir + 'house_prices/test.csv'
train = pd.read_csv(train_file)
test = pd.read_csv(test_file)
# 查看训练数据的特征
print(train.head(5))
# 查看测试数据的特征
print(test.head(5))

# 查看未删除ID之前数据的shape （1460， 81） （1459， 80）
print(&quot;The train data size before dropping Id feature is : {} &quot;.format(train.shape))
print(&quot;The test data size before dropping Id feature is : {} &quot;.format(test.shape))

# 保存 Id列的值
train_ID = train['Id']
test_ID = test['Id']

# 删除ID列的值，因为它对于预测结果没有太大的影响
train.drop(&quot;Id&quot;, axis = 1, inplace = True)
test.drop(&quot;Id&quot;, axis = 1, inplace = True)

# 检查删除ID以后的数据的shape （1460， 80） （1459， 79）
print(&quot;\nThe train data size after dropping Id feature is : {} &quot;.format(train.shape))
print(&quot;The test data size after dropping Id feature is : {} &quot;.format(test.shape))

# 删除那些异常数据值   异常值的处理对应于那些极端情况
'''
fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
这里举的是异常值的处理  Example：房子面积很大，但是价格很低 其他的这种异常并不是需要全部删除
还要保持模型的健壮性
'''
train = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)

# 对SalePrice使用log(1+x)的形式来处理
train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;])


# 特征工程
# 将train数据集和test数据集结合在一起
# ntrain ntest 各自行数
ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
# contact连接以后index只是重复，会出现逻辑错误  需要使用reset_index处理 drop为True
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print(&quot;all_data size is : {}&quot;.format(all_data.shape))

# 特征工程  对特征进行处理
# 处理缺失数据
print(all_data.isnull().sum())
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
print(missing_data.head(20))

all_data[&quot;PoolQC&quot;] = all_data[&quot;PoolQC&quot;].fillna(&quot;None&quot;)
all_data[&quot;MiscFeature&quot;] = all_data[&quot;MiscFeature&quot;].fillna(&quot;None&quot;)
all_data[&quot;Alley&quot;] = all_data[&quot;Alley&quot;].fillna(&quot;None&quot;)
all_data[&quot;Fence&quot;] = all_data[&quot;Fence&quot;].fillna(&quot;None&quot;)
all_data[&quot;FireplaceQu&quot;] = all_data[&quot;FireplaceQu&quot;].fillna(&quot;None&quot;)

# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
all_data[&quot;LotFrontage&quot;] = all_data.groupby(&quot;Neighborhood&quot;)[&quot;LotFrontage&quot;].transform(
    lambda x: x.fillna(x.median()))
for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
    all_data[col] = all_data[col].fillna('None')
for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    all_data[col] = all_data[col].fillna(0)
for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    all_data[col] = all_data[col].fillna(0)
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    all_data[col] = all_data[col].fillna('None')
all_data[&quot;MasVnrType&quot;] = all_data[&quot;MasVnrType&quot;].fillna(&quot;None&quot;)
all_data[&quot;MasVnrArea&quot;] = all_data[&quot;MasVnrArea&quot;].fillna(0)
all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])
# all records are &quot;AllPub&quot;, except for one &quot;NoSeWa&quot; and 2 NA  remove
all_data = all_data.drop(['Utilities'], axis=1)
all_data[&quot;Functional&quot;] = all_data[&quot;Functional&quot;].fillna(&quot;Typ&quot;)
all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])
all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])
all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])
all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])
all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])
all_data['MSSubClass'] = all_data['MSSubClass'].fillna(&quot;None&quot;)
# 检查是否还有缺失值存在 输出结果是没有缺失值存在
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
print(missing_data.head())
# 附加的特征工程
# 把一些数值变量分类
#MSSubClass=建筑物的类别
all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)

#Changing OverallCond into a categorical variable
all_data['OverallCond'] = all_data['OverallCond'].astype(str)

#Year and month sold are transformed into categorical features.
all_data['YrSold'] = all_data['YrSold'].astype(str)
all_data['MoSold'] = all_data['MoSold'].astype(str)

# 这些类别型的数据需要转换为数值型的数据
cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',
        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',
        'YrSold', 'MoSold')
# 将这些有类别的特征值(例如英文表示的额) 使用LabelEncoder变成分类的数值数据
for c in cols:
    lbl = LabelEncoder()
    lbl.fit(list(all_data[c].values))
    all_data[c] = lbl.transform(list(all_data[c].values))


# shape
print('Shape all_data: {}'.format(all_data.shape))

# 增加更多重要的特征
# 计算出房子总的面积  包含上下两层的面积 基础的面积  一层面积 二层面积
all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']

# Skewed features 将内容为数值型的特征列找出来  skewed为偏斜度的设置
numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index
# 检查所有数值特征的偏斜度
skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False) #降序排列
print(&quot;\nSkew in numerical features: \n&quot;)
skewness = pd.DataFrame({'Skew' :skewed_feats})
print(skewness.head(10))

# Box Cox Transformation of (highly) skewed features
# We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .
# Note that setting  λ=0  is equivalent to log1p used above for the target variable.
skewness = skewness[abs (skewness) &gt; 0.75]
print (&quot;There are {} skewed numerical features to Box Cox transform&quot;.format (skewness.shape[0]))

# 转换数据位正态分布，避免长尾分布现象的出现
from scipy.special import boxcox1p

skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    # all_data[feat] += 1
    all_data[feat] = boxcox1p(all_data[feat], lam)
# one-hot编码
all_data = pd.get_dummies(all_data)
print(all_data.shape)
'''
###########################
对数据处理以后新的训练集和测试集
###########################
'''
train = all_data[:ntrain]
test = all_data[ntrain:]
# train, test = train_test_split(all_data, train_size=0.8, random_state=2)

#k-fold交叉验证 5折交叉验证
n_folds = 5


def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=&quot;neg_mean_squared_error&quot;, cv=kf))
    return(rmse)

# 模型的选择和使用
# LASSO Regression :
lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))
# Elastic Net Regression 弹性网回归
ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))
# Kernel Ridge Regression 岭回归 alpha超参数
KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
# Gradient Boosting Regression 梯度增强回归
GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10,
                                   loss='huber', random_state =5)
#  XGboost
model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,
                             learning_rate=0.05, max_depth=3,
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)
# lightGBM
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
# 基础模型的评价得分  mean计算均值   std() 计算的是标准偏差
score = rmsle_cv(lasso)
print(&quot;\nLasso score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(ENet)
print(&quot;ElasticNet score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(KRR)
print(&quot;Kernel Ridge score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(GBoost)
print(&quot;Gradient Boosting score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(model_xgb)
print(&quot;Xgboost score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(model_lgb)
print(&quot;LGBM score: {:.4f} ({:.4f})\n&quot; .format(score.mean(), score.std()))


# 基础模型的分类器的平均值
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models
    # 模型的fit()函数
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)
        return self

    # 模型的预测 k-fold为预测的次数，将多列的数据合并，并取平均值
    def predict(self, X):
        # column_stack可以将多个列组合   [0,1],[2,3] 组合为 [[0,2],[1,3]]
        predictions = np.column_stack([model.predict (X) for model in self.models_])
        return np.mean(predictions, axis=1)
# 评价这四个模型的好坏
averaged_models = AveragingModels(models=(ENet, GBoost, KRR, lasso))
score = rmsle_cv(averaged_models)
print (&quot; Averaged base models score: {:.4f} ({:.4f})\n&quot;.format (score.mean (), score.std()))


# 增加Meta-model 最最精彩的部分***回归预测***
class StackingAveragedModels (BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds

    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list () for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)

        # 使用K-fold的方法来进行交叉验证，将每次验证的结果作为新的特征来进行处理
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index],  y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred

        # 将交叉验证预测出的结果 和 训练集中的标签值进行训练
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self

    # 从得到的新的特征  采用新的模型进行预测  并输出结果
    def predict(self, X):
        #两层列表解析，第一层为了生成kflod对应的模型组list，3（nbase_model）*5(nkfold)
        #第二层为三个basemodel的均值组成
        meta_features = np.column_stack ([
            np.column_stack([model.predict (X) for model in base_models]).mean (axis=1)
            for base_models in self.base_models_])
        return self.meta_model_.predict(meta_features)

stacked_averaged_models = StackingAveragedModels(base_models=(ENet, GBoost, KRR), # meta_model=model_lgb)
                                                 meta_model=lasso)
score = rmsle_cv(stacked_averaged_models)
print(&quot;Stacking Averaged models score: {:.4f} ({:.4f})&quot;.format(score.mean(), score.std()))
# 均方根误差评价函数
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

# 最后的Training、Prediction 输出结果
# StackedRegressor
stacked_averaged_models.fit(train.values, y_train)
stacked_train_pred = stacked_averaged_models.predict(train.values)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
print(rmsle(y_train, stacked_train_pred))

# XGBoost
model_xgb.fit(train, y_train)
xgb_train_pred = model_xgb.predict(train)
xgb_pred = np.expm1(model_xgb.predict(test))
print(rmsle(y_train, xgb_train_pred))

# lightGBM
model_lgb.fit(train, y_train)
lgb_train_pred = model_lgb.predict(train)
lgb_pred = np.expm1(model_lgb.predict(test.values))
print(rmsle(y_train, lgb_train_pred))

'''
RMSE on the entire Train data when averaging
'''
print('RMSLE score on train data:')
print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 ))
# 模型融合的预测效果
ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15
# 保存结果
result = pd.DataFrame()
result['Id'] = test_ID
result['SalePrice'] = ensemble
# index=False 是用来除去行编号
output_file_name = output_dir + 'house_price_stacking_rslt.csv'
result.to_csv(output_file_name, index=False)
print('##########结束训练##########')
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00003_TFRecords]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00003_tfrecords/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00003_tfrecords/">
        </link>
        <updated>2021-05-11T12:57:03.000Z</updated>
        <content type="html"><![CDATA[<p>import tensorflow_datasets as tfds</p>
<p>datasets = tfds.load(name=&quot;mnist&quot;)<br>
mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;]<br>
print(tfds.list_builders())</p>
<h1 id="加载fashion-mnist数据集将其拆分为训练集-验证集-测试集">加载Fashion MNIST数据集；将其拆分为训练集、验证集、测试集；</h1>
<p>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()<br>
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]<br>
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]</p>
<p>keras.backend.clear_session()<br>
np.random.seed(7)<br>
tf.random.set_seed(77)</p>
<h1 id="shuffle打乱测试集-训练集-验证集的顺序">shuffle打乱测试集、训练集、验证集的顺序</h1>
<p>train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))<br>
valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))<br>
test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))</p>
<p>def create_example(image, label):<br>
image_data = tf.io.serialize_tensor(image)<br>
#image_data = tf.io.encode_jpeg(image[..., np.newaxis])<br>
return Example(<br>
features=Features(<br>
feature={<br>
&quot;image&quot;: Feature(bytes_list=BytesList(value=[image_data.numpy()])),<br>
&quot;label&quot;: Feature(int64_list=Int64List(value=[label])),<br>
}))</p>
<p>for image, label in valid_set.take(1):<br>
print(create_example(image, label))</p>
<p>from contextlib import ExitStack</p>
<h1 id=""></h1>
<p>def write_tfrecords(name, dataset, n_shards=10):<br>
paths = [&quot;{}.tfrecord-{:05d}-of-{:05d}&quot;.format(name, index, n_shards)<br>
for index in range(n_shards)]<br>
with ExitStack() as stack:<br>
writers = [stack.enter_context(tf.io.TFRecordWriter(path))<br>
for path in paths]<br>
for index, (image, label) in dataset.enumerate():<br>
shard = index % n_shards<br>
example = create_example(image, label)<br>
writers[shard].write(example.SerializeToString())<br>
return paths</p>
<p>train_filepaths = write_tfrecords(&quot;my_fashion_mnist.train&quot;, train_set)<br>
valid_filepaths = write_tfrecords(&quot;my_fashion_mnist.valid&quot;, valid_set)<br>
test_filepaths = write_tfrecords(&quot;my_fashion_mnist.test&quot;, test_set)</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00002_TextVectorization]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/">
        </link>
        <updated>2021-05-09T10:11:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>如果类别小于10，通常采用独热编码方式。如果类别大于50，通常使用嵌入编码。在10到50中间你需要实践比对一番，那种方式更好。</p>
</blockquote>
<p>我们将文本文档转成数值型feature向量的过程称为<strong>向量化（vectorization）</strong>。这种特定的策略（tokenization/counting/normalization）被称为词袋（Bag of Words）或者”Bag of n-grams”表示。通过单词出现率文档描述的文档会完全忽略文档中单词的相对位置。</p>
<h5 id="词嵌入的作用对于原始数据一串符号不能直接传给算法必须将它们表示成使用固定size的数值型的feature向量而非变长的原始文档">词嵌入的作用:对于原始数据，一串符号不能直接传给算法，必须将它们表示成使用固定size的数值型的feature向量，而非变长的原始文档。</h5>
<p>1、使用独热编码分类特征</p>
<p>2、使用嵌入编码分类特征</p>
<p><strong>Feature extraction与Feature Selection是完全不同的：前者将专有数据（文本或图片）转换成机器学习中可用的数值型特征；后者则是选取特征上的技术</strong></p>
<p>为了这个表示，sklearn提供了相应的工具类来从文本内容中抽取数值型feature，对应的功能名为：</p>
<ul>
<li><strong>tokenizing</strong>：对strings进行token化，给每个可能的token一个integer id，例如：使用空格或逗豆做作token分割符。</li>
<li><strong>counting</strong>: 统计在每个文档中token的出现次数。</li>
<li><strong>normalizing和weighting</strong>：对在大多数样本/文档中出现的重要的token进行归一化和降权。</li>
</ul>
<p>在这种scheme下，features和samples的定义如下:</p>
<ul>
<li>每个独立的token出现率（不管是否归一化），都被当成一个feature。</li>
<li>对于一个给定的文档，所有的token频率的向量都被认为是一个<strong>多元样本（multivariate sample）</strong>。</li>
</ul>
<p>一个文档型语料可以被表示成一个矩阵：每个文档一行，每个在语料中出现的token一列（word）。</p>
<p>深度学习模型不会接收原始文本作为输入，它只能处理数值张量。文本向量化（vectorize）是指将文本转换为数值张量的过程。主要有三种实现方式：1.单词级；2字符级；3.提取单词或者字符的n-gram(多个连续的集合）<br>
将文本分解成的单元（单词、字符或 n-gram）叫作标记（token），将文本分解成标记的过程叫作分词（tokenization）。所有文本向量化过程都是应用某种分词方案，然后将数值向量与生成的标记相关联。这些向量组合成序列张量，被输入到深度神经网络中。将向量与标记相关联的主要方法有两种：做 one-hot 编码（one-hot encoding）与标记嵌入［token embedding，通常只用于单词，叫作词嵌入（word embedding）］。</p>
<h5 id="embedding-层">Embedding 层</h5>
<p><u>将 Embedding 层理解为一个字典，能将单词索引映射为密集向量。</u>它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding 层实际上是一种字典查找。</p>
<pre><code class="language-python">#  标记的个数（词汇表大小，这里是 1000，即最大单词索引 +1）和嵌入的维度（这里是 64,有64个特征组成一个单词）
from keras.layers improt Embedding
embedding_layer = Embedding(1000,64)

</code></pre>
<h5 id="embedding-层的输入和输出">Embedding 层的输入和输出：</h5>
<p>输入：一个二维张量，形状为（samples，sequential_length）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数，每个单词对应一个数字，一共sequential_length个单词。<br>
一批数据中的所有序列必须具有相同的长度（因为需要将它们打包成一个张量），短的补0，长的截断<br>
输出：一个三维张量，形状为（samples，sequential_length，dimensionality）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数<br>
dimensionality：代表通道数，同一samples，同一sequential_length上的所有通道上的值组成的向量表示一个单词，如（0，0，：）代表一个单词。</p>
<h5 id="embedding-层的学习和目的">Embedding 层的学习和目的：</h5>
<p>将一个 Embedding 层实例化时，它的权重（即标记向量的内部字典）最开始是随机的，与其他层一样。在训练过程中，利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以利用。一旦训练完成，嵌入空间将会展示大量结构，这种结构专门针对训练模型所要解决的问题。</p>
<h5 id="embedding-层的实践">Embedding 层的实践：</h5>
<p>场景：IMDB 电影评论情感预测任务<br>
方法：将电影评论限制为前 10 000 个最常见的单词（第一次处理这个数据集时就是这么做的），然后将评论长度限制为只有 20 个单词。对于这 10 000 个单词，网络将对每个词都学习一个 8维嵌入，将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量），然后将这个张量展平为二维，最后在上面训练一个 Dense 层用于分类</p>
<p>keras 版本：</p>
<pre><code class="language-python">from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding
 
&quot;&quot;&quot;1.获取数据作为embedding层的输入：将整数列表转换成形状为（samples,maxlen）的二维整数张量&quot;&quot;&quot;
max_features =10000 #前一万个单词组成词汇表，也就是ont-hot里面特征数
maxlen=20#输入的单词序列长度要相同
 
&quot;&quot;&quot;将数据加载为整数列表&quot;&quot;&quot;
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
&quot;&quot;&quot;输入前要截成相同长度&quot;&quot;&quot;
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 
&quot;&quot;&quot;2.使用Embedding 层和分类器&quot;&quot;&quot;
model=Sequential()
model.add(Embedding(10000,8,input_length=maxlen))
&quot;&quot;&quot;将三维的嵌入张量展平成形状为 (samples, maxlen * 8) 的二维张量：
一个sample的所有(maxlen个)单词所有（8个）特征,这就是为什么要截取一样的长度&quot;&quot;&quot;
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])
model.summary()#输出网络结构
#训练模型：自己切分来训练
history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)
</code></pre>
<p>tensorflow 版本：</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

assert tf.__version__ &gt;= &quot;2.0&quot;

from pathlib import Path
from collections import Counter

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
path = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/datasets/aclImdb'



# glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]

# 初始化str为路径 否则报错：AttributeError: 'str' object has no attribute 'glob'
path = Path(path)

train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# TextLineDataset读取csv拆分为neg和pos标签
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)


batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# 输入前要截取成相同程度的文本pad的作用也是保持每个token的单位长度
def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)


# 统计在每个文档中token的出现次数函数
def get_vocabulary(data_sample, max_size=1000):
    preprocessed_reviews = preprocess(data_sample).numpy()
    counter = Counter()
    for words in preprocessed_reviews:
        for word in words:
            if word != b&quot;&lt;pad&gt;&quot;:
                counter[word] += 1
    return [b&quot;&lt;pad&gt;&quot;] + [word for word, count in counter.most_common(max_size)]


get_vocabulary(X_example)


# 1、preprocess 生成 token
# 2、get_vocabulary 统计频率
# 将token统计频率后的形成table与indx接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量
class TextVectorization(keras.layers.Layer):
    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.max_vocabulary_size = max_vocabulary_size
        self.n_oov_buckets = n_oov_buckets

    def adapt(self, data_sample):
        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)
        words = tf.constant(self.vocab)
        word_ids = tf.range(len(self.vocab), dtype=tf.int64)
        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)

    def call(self, inputs):
        preprocessed_inputs = preprocess(inputs)
        return self.table.lookup(preprocessed_inputs)


text_vectorization = TextVectorization()

text_vectorization.adapt(X_example)
text_vectorization(X_example)

max_vocabulary_size = 1000
n_oov_buckets = 100

sample_review_batches = train_set.map(lambda review, label: review)
sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),
                                axis=0)

text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,
                                       input_shape=[])
text_vectorization.adapt(sample_reviews)
text_vectorization(X_example)

simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])
tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)

print('text_vectorization.vocab: ', text_vectorization.vocab[:10])


class BagOfWords(keras.layers.Layer):
    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.n_tokens = n_tokens

    def call(self, inputs):
        one_hot = tf.one_hot(inputs, self.n_tokens)
        return tf.reduce_sum(one_hot, axis=1)[:, 1:]


bag_of_words = BagOfWords(n_tokens=4)
print('bag_of_words(simple_example): ', bag_of_words(simple_example))

n_tokens = max_vocabulary_size + n_oov_buckets + 1  # add 1 for &lt;pad&gt;
bag_of_words = BagOfWords(n_tokens)

model = keras.models.Sequential([
    text_vectorization,
    bag_of_words,
    keras.layers.Dense(100, activation=&quot;relu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;),
])
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])
model.fit(train_set, epochs=5, validation_data=valid_set)


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00001]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00001/">
        </link>
        <updated>2021-05-08T01:31:11.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>Task：对数据集进行拆分，创建一个tf.data.Dataset来加载并进行预处理，然后创建和训练一个包含Embedding层的二进制分类模型</p>
</blockquote>
<p>1.下载大型电影评论数据集；</p>
<p>2.将测试集拆分为一个验证集和一个测试集</p>
<p>3.创建一个二进制分类模型，使用TextVectorization层对每个评论进行预处理。如果TextVectorization层尚不可用，请自定义预处理层。</p>
<pre><code># TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

from IPython import get_ipython

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# ## 10.
# _Exercise: In this exercise you will download a dataset, split it,
# create a `tf.data.Dataset` to load it and preprocess it efficiently,
# then build and train a binary classification model containing an `Embedding` layer._
#
# ### a.
# _Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb),
# which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/).
# The data is organized in two directories, `train` and `test`,
# each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews.
# Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words),
# but we will ignore them in this exercise._




from pathlib import Path

# 使用keras.utils.get_file下载imdb数据集，cache_dir指定自己的文件目录；
DOWNLOAD_ROOT = &quot;http://ai.stanford.edu/~amaas/data/sentiment/&quot;
FILENAME = &quot;aclImdb_v1.tar.gz&quot;
filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True,cache_dir=input_dir)
path = Path(filepath).parent / &quot;aclImdb&quot;
print(path)
# C:\Users\ericf\PycharmProjects\data_scientenist\data\raw\datasets\aclImdb

# 打印数据集的层级关系和文件名
#  os.walk根目录下的每一个文件夹(包含它自己), 产生3-元组 (dirpath, dirnames, filenames)【文件夹路径, 文件夹名字, 文件名】
#  name, subdirs, files=【文件夹路径, 文件夹名字, 文件名】
#  indent 根据文件夹目录记录缩进
for name, subdirs, files in os.walk(path):
    indent = len(Path(name).parts) - len(path.parts)
    print(&quot;    &quot; * indent + Path(name).parts[-1] + os.sep)
    for index, filename in enumerate(sorted(files)):
        if index == 3:
            print(&quot;    &quot; * (indent + 1) + &quot;...&quot;)
            break
        print(&quot;    &quot; * (indent + 1) + filename)

#glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]


train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

print(len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg))

# ### b.
# _Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._
# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# ### c.
# _Exercise: Use tf.data to create an efficient dataset for each set._

# Since the dataset fits in memory,
# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:

# 对于影评进行二分类标签处理
def imdb_dataset(filepaths_positive, filepaths_negative):
    reviews = []
    labels = []
    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):
        for filepath in filepaths:
            with open(filepath,encoding='utf-8') as review_file:
                reviews.append(review_file.read())
            labels.append(label)
    return tf.data.Dataset.from_tensor_slices(
        (tf.constant(reviews), tf.constant(labels)))



for X, y in imdb_dataset(train_pos, train_neg).take(3):
    print(X)
    print(y)
    print()

#get_ipython().run_line_magic('timeit', '-r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass')


# It takes about 17 seconds to load the dataset and go through it 10 times.

# But let's pretend the dataset does not fit in memory, just to make things more interesting.
# Luckily, each review fits on just one line (they use `&lt;br /&gt;` to indicate line breaks),
# so we can read the reviews using a `TextLineDataset`.
# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).
# For very large datasets, it would make sense to use a tool like Apache Beam for that.


# 1.tf.data.TextLineDataset 读取数据集，自动创建dataset
# 2.读取之后使用map根据pos和neg进行打标签操作
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)




# Now it takes about 33 seconds to go through the dataset 10 times. That's much slower,
# essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.
# If you add `.cache()` just before `.repeat(10)`,
# you will see that this implementation will be about as fast as the previous one.



batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# ### d.
# _Exercise: Create a binary classification model,
# using a `TextVectorization` layer to preprocess each review.
# If the `TextVectorization` layer is not yet available (or if you like a challenge),
# try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package,
# for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces,
# and `split()` to split words on spaces.
# You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._

# Let's first write a function to preprocess the reviews, cropping them to 300 characters,
# converting them to lower case, then replacing `&lt;br /&gt;` and all non-letter characters to spaces,
# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:




def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSK_999]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_-xue-_001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">
        </link>
        <updated>2021-05-07T08:33:11.000Z</updated>
        <content type="html"><![CDATA[<p>#Adaboost、GBDT、XGBoost的区别是什么？</p>
<p>Gradient Boosting 和其它 Boosting 算法一样，通过将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说 Gradient Boosting = Gradient Descent + Boosting。</p>
<p>和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost 是通过提升错分数据点的权重来定位模型的不足而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost, Gradient Boosting 可以使用更多种类的目标函数。</p>
<p><strong>Adaboost VS GBDT最主要的区别在于两者如何识别模型的问题</strong>。AdaBoost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。Gradient Boosting通过负梯度来识别问题，通过计算负梯度来改进模型。</p>
<p>#GBDT 与 XGBoost 区别<br>
传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。<br>
传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p>
<p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p>
]]></content>
    </entry>
</feed>