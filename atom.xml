<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://fa7ria.GitHub.io</id>
    <title>Fa7riaBlog</title>
    <updated>2021-05-23T01:05:46.572Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://fa7ria.GitHub.io"/>
    <link rel="self" href="https://fa7ria.GitHub.io/atom.xml"/>
    <subtitle>ä¸ºå­¦æ—¥ç›Šï¼Œä¸ºé“æ—¥æŸã€‚æŸä¹‹åˆæŸï¼Œä»¥è‡³äºæ— ä¸ºï¼Œæ— ä¸ºè€Œæ— ä¸ä¸º</subtitle>
    <logo>https://fa7ria.GitHub.io/images/avatar.png</logo>
    <icon>https://fa7ria.GitHub.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Fa7riaBlog</rights>
    <entry>
        <title type="html"><![CDATA[DSP_00005_Stacking_house_price]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00005_stacking_house_price/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00005_stacking_house_price/">
        </link>
        <updated>2021-05-23T01:04:50.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<pre><code>Stacking æ˜¯ä»€ä¹ˆ?
Stackingç®€å•ç†è§£å°±æ˜¯è®²å‡ ä¸ªç®€å•çš„æ¨¡å‹ï¼Œä¸€èˆ¬é‡‡ç”¨å°†å®ƒä»¬è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯è¾“å‡ºé¢„æµ‹ç»“æœï¼Œ ç„¶åå°†æ¯ä¸ªæ¨¡å‹è¾“å‡ºçš„é¢„æµ‹ç»“æœåˆå¹¶ä¸ºæ–°çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ–°çš„æ¨¡å‹åŠ ä»¥è®­ç»ƒã€‚
</code></pre>
</blockquote>
<pre><code class="language-python"># -*- coding: utf-8 -*-


'''
æè¿°ï¼šDataScientist_Practice_00005
    hands-on ml charpt13 practice
    Stacking æ˜¯ä»€ä¹ˆ?
ä½œè€…ï¼šliuyi
ç¨‹åºå¼€å‘ç¯å¢ƒï¼šwin 64ä½
Pythonç‰ˆæœ¬ï¼š64ä½ 3.8.3ï¼ˆä½¿ç”¨Anacondaå®‰è£…ï¼‰
python IDEï¼šPyCharm 2020.1ä¸“ä¸šç‰ˆ
ä¾èµ–åº“ï¼štensorflow,keras,IPython,numpy,os
ç¨‹åºè¾“å…¥ï¼šå…·ä½“æŸ¥çœ‹å„æ¨¡å—
ç¨‹åºè¾“å‡ºï¼šå…·ä½“æŸ¥çœ‹å„æ¨¡å—

'''
####################################################################





# åŠ è½½æ•°æ®é›†
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# Stacking æ˜¯ä»€ä¹ˆ?
# Stackingç®€å•ç†è§£å°±æ˜¯è®²å‡ ä¸ªç®€å•çš„æ¨¡å‹ï¼Œä¸€èˆ¬é‡‡ç”¨å°†å®ƒä»¬è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯è¾“å‡ºé¢„æµ‹ç»“æœï¼Œ
# ç„¶åå°†æ¯ä¸ªæ¨¡å‹è¾“å‡ºçš„é¢„æµ‹ç»“æœåˆå¹¶ä¸ºæ–°çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ–°çš„æ¨¡å‹åŠ ä»¥è®­ç»ƒã€‚
#
# ###
# ä¸»è¦çš„è¿‡ç¨‹ç®€è¿°å¦‚ä¸‹ï¼š
# 1ã€é¦–å…ˆéœ€è¦å‡ ä¸ªæ¨¡å‹ï¼Œç„¶åå¯¹å·²æœ‰çš„æ•°æ®é›†è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯
# 2ã€KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒé›†ï¼Œå¯¹æ¯æŠ˜çš„è¾“å‡ºç»“æœä¿å­˜ï¼Œæœ€åè¿›è¡Œåˆå¹¶
# 3ã€å¯¹äºæµ‹è¯•é›†T1çš„å¾—åˆ°ï¼Œæœ‰ä¸¤ç§æ–¹æ³•ã€‚æ³¨æ„åˆ°åˆšåˆšæ˜¯2æŠ˜äº¤å‰éªŒè¯ï¼ŒM1ç›¸å½“äºè®­ç»ƒäº†2æ¬¡ï¼Œ
#  æ‰€ä»¥ä¸€ç§æ–¹æ³•æ˜¯æ¯ä¸€æ¬¡è®­ç»ƒM1ï¼Œå¯ä»¥ç›´æ¥å¯¹æ•´ä¸ªtestè¿›è¡Œé¢„æµ‹ï¼Œè¿™æ ·2æŠ˜äº¤å‰éªŒè¯åæµ‹è¯•é›†ç›¸å½“äºé¢„æµ‹äº†2æ¬¡ï¼Œç„¶åå¯¹è¿™ä¸¤åˆ—æ±‚å¹³å‡å¾—åˆ°T1ã€‚
# 4ã€æ˜¯ä¸¤å±‚å¾ªç¯ï¼Œç¬¬ä¸€å±‚å¾ªç¯æ§åˆ¶åŸºæ¨¡å‹çš„æ•°ç›®ï¼Œç¬¬äºŒå±‚å¾ªç¯æ§åˆ¶çš„æ˜¯äº¤å‰éªŒè¯çš„æ¬¡æ•°Kï¼Œå¯¹æ¯ä¸€ä¸ªåŸºæ¨¡å‹ä¼šè®­ç»ƒKæ¬¡ï¼Œç„¶åæ‹¼æ¥å¾—åˆ°é¢„æµ‹ç»“æœP1ã€‚
# 5ã€è¯¥å›¾æ˜¯ä¸€ä¸ªåŸºæ¨¡å‹å¾—åˆ°P1å’ŒT1çš„è¿‡ç¨‹ï¼Œé‡‡ç”¨çš„æ˜¯5æŠ˜äº¤å‰éªŒè¯ï¼Œæ‰€ä»¥å¾ªç¯äº†5æ¬¡ï¼Œæ‹¼æ¥å¾—åˆ°P1ï¼Œæµ‹è¯•é›†é¢„æµ‹äº†5æ¬¡ï¼Œå–å¹³å‡å¾—åˆ°T1ã€‚
#   è€Œè¿™ä»…ä»…åªæ˜¯ç¬¬äºŒå±‚è¾“å…¥çš„ä¸€åˆ—/ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶ä¸æ˜¯æ•´ä¸ªè®­ç»ƒé›†ã€‚å†åˆ†æä½œè€…çš„ä»£ç ä¹Ÿå°±å¾ˆæ¸…æ¥šäº†ã€‚ä¹Ÿå°±æ˜¯åˆšåˆšæåˆ°çš„ä¸¤å±‚å¾ªç¯
#import some necessary librairies

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
# ignore annoying warning (from sklearn and seaborn)
warnings.warn = ignore_warn
from sklearn.preprocessing import LabelEncoder
from scipy import stats
from scipy.stats import norm, skew #for some statistics
from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb

# è®¾ç½®è¾“å‡ºçš„æµ®ç‚¹æ•°çš„æ ¼å¼
pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))

from subprocess import check_output
# check the files available in the directory
# print(check_output([&quot;ls&quot;, &quot;/Users/liudong/Desktop/house_price/train.csv&quot;]).decode(&quot;utf8&quot;))
# åŠ è½½æ•°æ®
train_file = input_dir + 'house_prices/train.csv'
test_file = input_dir + 'house_prices/test.csv'
train = pd.read_csv(train_file)
test = pd.read_csv(test_file)
# æŸ¥çœ‹è®­ç»ƒæ•°æ®çš„ç‰¹å¾
print(train.head(5))
# æŸ¥çœ‹æµ‹è¯•æ•°æ®çš„ç‰¹å¾
print(test.head(5))

# æŸ¥çœ‹æœªåˆ é™¤IDä¹‹å‰æ•°æ®çš„shape ï¼ˆ1460ï¼Œ 81ï¼‰ ï¼ˆ1459ï¼Œ 80ï¼‰
print(&quot;The train data size before dropping Id feature is : {} &quot;.format(train.shape))
print(&quot;The test data size before dropping Id feature is : {} &quot;.format(test.shape))

# ä¿å­˜ Idåˆ—çš„å€¼
train_ID = train['Id']
test_ID = test['Id']

# åˆ é™¤IDåˆ—çš„å€¼ï¼Œå› ä¸ºå®ƒå¯¹äºé¢„æµ‹ç»“æœæ²¡æœ‰å¤ªå¤§çš„å½±å“
train.drop(&quot;Id&quot;, axis = 1, inplace = True)
test.drop(&quot;Id&quot;, axis = 1, inplace = True)

# æ£€æŸ¥åˆ é™¤IDä»¥åçš„æ•°æ®çš„shape ï¼ˆ1460ï¼Œ 80ï¼‰ ï¼ˆ1459ï¼Œ 79ï¼‰
print(&quot;\nThe train data size after dropping Id feature is : {} &quot;.format(train.shape))
print(&quot;The test data size after dropping Id feature is : {} &quot;.format(test.shape))

# åˆ é™¤é‚£äº›å¼‚å¸¸æ•°æ®å€¼   å¼‚å¸¸å€¼çš„å¤„ç†å¯¹åº”äºé‚£äº›æç«¯æƒ…å†µ
'''
fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
è¿™é‡Œä¸¾çš„æ˜¯å¼‚å¸¸å€¼çš„å¤„ç†  Exampleï¼šæˆ¿å­é¢ç§¯å¾ˆå¤§ï¼Œä½†æ˜¯ä»·æ ¼å¾ˆä½ å…¶ä»–çš„è¿™ç§å¼‚å¸¸å¹¶ä¸æ˜¯éœ€è¦å…¨éƒ¨åˆ é™¤
è¿˜è¦ä¿æŒæ¨¡å‹çš„å¥å£®æ€§
'''
train = train.drop(train[(train['GrLivArea']&gt;4000) &amp; (train['SalePrice']&lt;300000)].index)

# å¯¹SalePriceä½¿ç”¨log(1+x)çš„å½¢å¼æ¥å¤„ç†
train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;])


# ç‰¹å¾å·¥ç¨‹
# å°†trainæ•°æ®é›†å’Œtestæ•°æ®é›†ç»“åˆåœ¨ä¸€èµ·
# ntrain ntest å„è‡ªè¡Œæ•°
ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
# contactè¿æ¥ä»¥åindexåªæ˜¯é‡å¤ï¼Œä¼šå‡ºç°é€»è¾‘é”™è¯¯  éœ€è¦ä½¿ç”¨reset_indexå¤„ç† dropä¸ºTrue
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print(&quot;all_data size is : {}&quot;.format(all_data.shape))

# ç‰¹å¾å·¥ç¨‹  å¯¹ç‰¹å¾è¿›è¡Œå¤„ç†
# å¤„ç†ç¼ºå¤±æ•°æ®
print(all_data.isnull().sum())
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
print(missing_data.head(20))

all_data[&quot;PoolQC&quot;] = all_data[&quot;PoolQC&quot;].fillna(&quot;None&quot;)
all_data[&quot;MiscFeature&quot;] = all_data[&quot;MiscFeature&quot;].fillna(&quot;None&quot;)
all_data[&quot;Alley&quot;] = all_data[&quot;Alley&quot;].fillna(&quot;None&quot;)
all_data[&quot;Fence&quot;] = all_data[&quot;Fence&quot;].fillna(&quot;None&quot;)
all_data[&quot;FireplaceQu&quot;] = all_data[&quot;FireplaceQu&quot;].fillna(&quot;None&quot;)

# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
all_data[&quot;LotFrontage&quot;] = all_data.groupby(&quot;Neighborhood&quot;)[&quot;LotFrontage&quot;].transform(
    lambda x: x.fillna(x.median()))
for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
    all_data[col] = all_data[col].fillna('None')
for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    all_data[col] = all_data[col].fillna(0)
for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    all_data[col] = all_data[col].fillna(0)
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    all_data[col] = all_data[col].fillna('None')
all_data[&quot;MasVnrType&quot;] = all_data[&quot;MasVnrType&quot;].fillna(&quot;None&quot;)
all_data[&quot;MasVnrArea&quot;] = all_data[&quot;MasVnrArea&quot;].fillna(0)
all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])
# all records are &quot;AllPub&quot;, except for one &quot;NoSeWa&quot; and 2 NA  remove
all_data = all_data.drop(['Utilities'], axis=1)
all_data[&quot;Functional&quot;] = all_data[&quot;Functional&quot;].fillna(&quot;Typ&quot;)
all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])
all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])
all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])
all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])
all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])
all_data['MSSubClass'] = all_data['MSSubClass'].fillna(&quot;None&quot;)
# æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼å­˜åœ¨ è¾“å‡ºç»“æœæ˜¯æ²¡æœ‰ç¼ºå¤±å€¼å­˜åœ¨
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
print(missing_data.head())
# é™„åŠ çš„ç‰¹å¾å·¥ç¨‹
# æŠŠä¸€äº›æ•°å€¼å˜é‡åˆ†ç±»
#MSSubClass=å»ºç­‘ç‰©çš„ç±»åˆ«
all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)

#Changing OverallCond into a categorical variable
all_data['OverallCond'] = all_data['OverallCond'].astype(str)

#Year and month sold are transformed into categorical features.
all_data['YrSold'] = all_data['YrSold'].astype(str)
all_data['MoSold'] = all_data['MoSold'].astype(str)

# è¿™äº›ç±»åˆ«å‹çš„æ•°æ®éœ€è¦è½¬æ¢ä¸ºæ•°å€¼å‹çš„æ•°æ®
cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',
        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',
        'YrSold', 'MoSold')
# å°†è¿™äº›æœ‰ç±»åˆ«çš„ç‰¹å¾å€¼(ä¾‹å¦‚è‹±æ–‡è¡¨ç¤ºçš„é¢) ä½¿ç”¨LabelEncoderå˜æˆåˆ†ç±»çš„æ•°å€¼æ•°æ®
for c in cols:
    lbl = LabelEncoder()
    lbl.fit(list(all_data[c].values))
    all_data[c] = lbl.transform(list(all_data[c].values))


# shape
print('Shape all_data: {}'.format(all_data.shape))

# å¢åŠ æ›´å¤šé‡è¦çš„ç‰¹å¾
# è®¡ç®—å‡ºæˆ¿å­æ€»çš„é¢ç§¯  åŒ…å«ä¸Šä¸‹ä¸¤å±‚çš„é¢ç§¯ åŸºç¡€çš„é¢ç§¯  ä¸€å±‚é¢ç§¯ äºŒå±‚é¢ç§¯
all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']

# Skewed features å°†å†…å®¹ä¸ºæ•°å€¼å‹çš„ç‰¹å¾åˆ—æ‰¾å‡ºæ¥  skewedä¸ºåæ–œåº¦çš„è®¾ç½®
numeric_feats = all_data.dtypes[all_data.dtypes != &quot;object&quot;].index
# æ£€æŸ¥æ‰€æœ‰æ•°å€¼ç‰¹å¾çš„åæ–œåº¦
skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False) #é™åºæ’åˆ—
print(&quot;\nSkew in numerical features: \n&quot;)
skewness = pd.DataFrame({'Skew' :skewed_feats})
print(skewness.head(10))

# Box Cox Transformation of (highly) skewed features
# We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .
# Note that setting  Î»=0  is equivalent to log1p used above for the target variable.
skewness = skewness[abs (skewness) &gt; 0.75]
print (&quot;There are {} skewed numerical features to Box Cox transform&quot;.format (skewness.shape[0]))

# è½¬æ¢æ•°æ®ä½æ­£æ€åˆ†å¸ƒï¼Œé¿å…é•¿å°¾åˆ†å¸ƒç°è±¡çš„å‡ºç°
from scipy.special import boxcox1p

skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    # all_data[feat] += 1
    all_data[feat] = boxcox1p(all_data[feat], lam)
# one-hotç¼–ç 
all_data = pd.get_dummies(all_data)
print(all_data.shape)
'''
###########################
å¯¹æ•°æ®å¤„ç†ä»¥åæ–°çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
###########################
'''
train = all_data[:ntrain]
test = all_data[ntrain:]
# train, test = train_test_split(all_data, train_size=0.8, random_state=2)

#k-foldäº¤å‰éªŒè¯ 5æŠ˜äº¤å‰éªŒè¯
n_folds = 5


def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=&quot;neg_mean_squared_error&quot;, cv=kf))
    return(rmse)

# æ¨¡å‹çš„é€‰æ‹©å’Œä½¿ç”¨
# LASSO Regression :
lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))
# Elastic Net Regression å¼¹æ€§ç½‘å›å½’
ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))
# Kernel Ridge Regression å²­å›å½’ alphaè¶…å‚æ•°
KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
# Gradient Boosting Regression æ¢¯åº¦å¢å¼ºå›å½’
GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10,
                                   loss='huber', random_state =5)
#  XGboost
model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,
                             learning_rate=0.05, max_depth=3,
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)
# lightGBM
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
# åŸºç¡€æ¨¡å‹çš„è¯„ä»·å¾—åˆ†  meanè®¡ç®—å‡å€¼   std() è®¡ç®—çš„æ˜¯æ ‡å‡†åå·®
score = rmsle_cv(lasso)
print(&quot;\nLasso score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(ENet)
print(&quot;ElasticNet score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(KRR)
print(&quot;Kernel Ridge score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(GBoost)
print(&quot;Gradient Boosting score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(model_xgb)
print(&quot;Xgboost score: {:.4f} ({:.4f})\n&quot;.format(score.mean(), score.std()))
score = rmsle_cv(model_lgb)
print(&quot;LGBM score: {:.4f} ({:.4f})\n&quot; .format(score.mean(), score.std()))


# åŸºç¡€æ¨¡å‹çš„åˆ†ç±»å™¨çš„å¹³å‡å€¼
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models
    # æ¨¡å‹çš„fit()å‡½æ•°
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)
        return self

    # æ¨¡å‹çš„é¢„æµ‹ k-foldä¸ºé¢„æµ‹çš„æ¬¡æ•°ï¼Œå°†å¤šåˆ—çš„æ•°æ®åˆå¹¶ï¼Œå¹¶å–å¹³å‡å€¼
    def predict(self, X):
        # column_stackå¯ä»¥å°†å¤šä¸ªåˆ—ç»„åˆ   [0,1],[2,3] ç»„åˆä¸º [[0,2],[1,3]]
        predictions = np.column_stack([model.predict (X) for model in self.models_])
        return np.mean(predictions, axis=1)
# è¯„ä»·è¿™å››ä¸ªæ¨¡å‹çš„å¥½å
averaged_models = AveragingModels(models=(ENet, GBoost, KRR, lasso))
score = rmsle_cv(averaged_models)
print (&quot; Averaged base models score: {:.4f} ({:.4f})\n&quot;.format (score.mean (), score.std()))


# å¢åŠ Meta-model æœ€æœ€ç²¾å½©çš„éƒ¨åˆ†***å›å½’é¢„æµ‹***
class StackingAveragedModels (BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds

    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list () for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)

        # ä½¿ç”¨K-foldçš„æ–¹æ³•æ¥è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå°†æ¯æ¬¡éªŒè¯çš„ç»“æœä½œä¸ºæ–°çš„ç‰¹å¾æ¥è¿›è¡Œå¤„ç†
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index],  y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred

        # å°†äº¤å‰éªŒè¯é¢„æµ‹å‡ºçš„ç»“æœ å’Œ è®­ç»ƒé›†ä¸­çš„æ ‡ç­¾å€¼è¿›è¡Œè®­ç»ƒ
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self

    # ä»å¾—åˆ°çš„æ–°çš„ç‰¹å¾  é‡‡ç”¨æ–°çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹  å¹¶è¾“å‡ºç»“æœ
    def predict(self, X):
        #ä¸¤å±‚åˆ—è¡¨è§£æï¼Œç¬¬ä¸€å±‚ä¸ºäº†ç”Ÿæˆkflodå¯¹åº”çš„æ¨¡å‹ç»„listï¼Œ3ï¼ˆnbase_modelï¼‰*5(nkfold)
        #ç¬¬äºŒå±‚ä¸ºä¸‰ä¸ªbasemodelçš„å‡å€¼ç»„æˆ
        meta_features = np.column_stack ([
            np.column_stack([model.predict (X) for model in base_models]).mean (axis=1)
            for base_models in self.base_models_])
        return self.meta_model_.predict(meta_features)

stacked_averaged_models = StackingAveragedModels(base_models=(ENet, GBoost, KRR), # meta_model=model_lgb)
                                                 meta_model=lasso)
score = rmsle_cv(stacked_averaged_models)
print(&quot;Stacking Averaged models score: {:.4f} ({:.4f})&quot;.format(score.mean(), score.std()))
# å‡æ–¹æ ¹è¯¯å·®è¯„ä»·å‡½æ•°
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

# æœ€åçš„Trainingã€Prediction è¾“å‡ºç»“æœ
# StackedRegressor
stacked_averaged_models.fit(train.values, y_train)
stacked_train_pred = stacked_averaged_models.predict(train.values)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
print(rmsle(y_train, stacked_train_pred))

# XGBoost
model_xgb.fit(train, y_train)
xgb_train_pred = model_xgb.predict(train)
xgb_pred = np.expm1(model_xgb.predict(test))
print(rmsle(y_train, xgb_train_pred))

# lightGBM
model_lgb.fit(train, y_train)
lgb_train_pred = model_lgb.predict(train)
lgb_pred = np.expm1(model_lgb.predict(test.values))
print(rmsle(y_train, lgb_train_pred))

'''
RMSE on the entire Train data when averaging
'''
print('RMSLE score on train data:')
print(rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 ))
# æ¨¡å‹èåˆçš„é¢„æµ‹æ•ˆæœ
ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15
# ä¿å­˜ç»“æœ
result = pd.DataFrame()
result['Id'] = test_ID
result['SalePrice'] = ensemble
# index=False æ˜¯ç”¨æ¥é™¤å»è¡Œç¼–å·
output_file_name = output_dir + 'house_price_stacking_rslt.csv'
result.to_csv(output_file_name, index=False)
print('##########ç»“æŸè®­ç»ƒ##########')
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00003_TFRecords]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00003_tfrecords/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00003_tfrecords/">
        </link>
        <updated>2021-05-11T12:57:03.000Z</updated>
        <content type="html"><![CDATA[<p>import tensorflow_datasets as tfds</p>
<p>datasets = tfds.load(name=&quot;mnist&quot;)<br>
mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;]<br>
print(tfds.list_builders())</p>
<h1 id="åŠ è½½fashion-mnistæ•°æ®é›†å°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›†">åŠ è½½Fashion MNISTæ•°æ®é›†ï¼›å°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼›</h1>
<p>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()<br>
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]<br>
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]</p>
<p>keras.backend.clear_session()<br>
np.random.seed(7)<br>
tf.random.set_seed(77)</p>
<h1 id="shuffleæ‰“ä¹±æµ‹è¯•é›†-è®­ç»ƒé›†-éªŒè¯é›†çš„é¡ºåº">shuffleæ‰“ä¹±æµ‹è¯•é›†ã€è®­ç»ƒé›†ã€éªŒè¯é›†çš„é¡ºåº</h1>
<p>train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))<br>
valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))<br>
test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))</p>
<p>def create_example(image, label):<br>
image_data = tf.io.serialize_tensor(image)<br>
#image_data = tf.io.encode_jpeg(image[..., np.newaxis])<br>
return Example(<br>
features=Features(<br>
feature={<br>
&quot;image&quot;: Feature(bytes_list=BytesList(value=[image_data.numpy()])),<br>
&quot;label&quot;: Feature(int64_list=Int64List(value=[label])),<br>
}))</p>
<p>for image, label in valid_set.take(1):<br>
print(create_example(image, label))</p>
<p>from contextlib import ExitStack</p>
<h1 id=""></h1>
<p>def write_tfrecords(name, dataset, n_shards=10):<br>
paths = [&quot;{}.tfrecord-{:05d}-of-{:05d}&quot;.format(name, index, n_shards)<br>
for index in range(n_shards)]<br>
with ExitStack() as stack:<br>
writers = [stack.enter_context(tf.io.TFRecordWriter(path))<br>
for path in paths]<br>
for index, (image, label) in dataset.enumerate():<br>
shard = index % n_shards<br>
example = create_example(image, label)<br>
writers[shard].write(example.SerializeToString())<br>
return paths</p>
<p>train_filepaths = write_tfrecords(&quot;my_fashion_mnist.train&quot;, train_set)<br>
valid_filepaths = write_tfrecords(&quot;my_fashion_mnist.valid&quot;, valid_set)<br>
test_filepaths = write_tfrecords(&quot;my_fashion_mnist.test&quot;, test_set)</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00002_TextVectorization]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/">
        </link>
        <updated>2021-05-09T10:11:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>å¦‚æœç±»åˆ«å°äº10ï¼Œé€šå¸¸é‡‡ç”¨ç‹¬çƒ­ç¼–ç æ–¹å¼ã€‚å¦‚æœç±»åˆ«å¤§äº50ï¼Œé€šå¸¸ä½¿ç”¨åµŒå…¥ç¼–ç ã€‚åœ¨10åˆ°50ä¸­é—´ä½ éœ€è¦å®è·µæ¯”å¯¹ä¸€ç•ªï¼Œé‚£ç§æ–¹å¼æ›´å¥½ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬å°†æ–‡æœ¬æ–‡æ¡£è½¬æˆæ•°å€¼å‹featureå‘é‡çš„è¿‡ç¨‹ç§°ä¸º<strong>å‘é‡åŒ–ï¼ˆvectorizationï¼‰</strong>ã€‚è¿™ç§ç‰¹å®šçš„ç­–ç•¥ï¼ˆtokenization/counting/normalizationï¼‰è¢«ç§°ä¸ºè¯è¢‹ï¼ˆBag of Wordsï¼‰æˆ–è€…â€Bag of n-gramsâ€è¡¨ç¤ºã€‚é€šè¿‡å•è¯å‡ºç°ç‡æ–‡æ¡£æè¿°çš„æ–‡æ¡£ä¼šå®Œå…¨å¿½ç•¥æ–‡æ¡£ä¸­å•è¯çš„ç›¸å¯¹ä½ç½®ã€‚</p>
<h5 id="è¯åµŒå…¥çš„ä½œç”¨å¯¹äºåŸå§‹æ•°æ®ä¸€ä¸²ç¬¦å·ä¸èƒ½ç›´æ¥ä¼ ç»™ç®—æ³•å¿…é¡»å°†å®ƒä»¬è¡¨ç¤ºæˆä½¿ç”¨å›ºå®šsizeçš„æ•°å€¼å‹çš„featureå‘é‡è€Œéå˜é•¿çš„åŸå§‹æ–‡æ¡£">è¯åµŒå…¥çš„ä½œç”¨:å¯¹äºåŸå§‹æ•°æ®ï¼Œä¸€ä¸²ç¬¦å·ä¸èƒ½ç›´æ¥ä¼ ç»™ç®—æ³•ï¼Œå¿…é¡»å°†å®ƒä»¬è¡¨ç¤ºæˆä½¿ç”¨å›ºå®šsizeçš„æ•°å€¼å‹çš„featureå‘é‡ï¼Œè€Œéå˜é•¿çš„åŸå§‹æ–‡æ¡£ã€‚</h5>
<p>1ã€ä½¿ç”¨ç‹¬çƒ­ç¼–ç åˆ†ç±»ç‰¹å¾</p>
<p>2ã€ä½¿ç”¨åµŒå…¥ç¼–ç åˆ†ç±»ç‰¹å¾</p>
<p><strong>Feature extractionä¸Feature Selectionæ˜¯å®Œå…¨ä¸åŒçš„ï¼šå‰è€…å°†ä¸“æœ‰æ•°æ®ï¼ˆæ–‡æœ¬æˆ–å›¾ç‰‡ï¼‰è½¬æ¢æˆæœºå™¨å­¦ä¹ ä¸­å¯ç”¨çš„æ•°å€¼å‹ç‰¹å¾ï¼›åè€…åˆ™æ˜¯é€‰å–ç‰¹å¾ä¸Šçš„æŠ€æœ¯</strong></p>
<p>ä¸ºäº†è¿™ä¸ªè¡¨ç¤ºï¼Œsklearnæä¾›äº†ç›¸åº”çš„å·¥å…·ç±»æ¥ä»æ–‡æœ¬å†…å®¹ä¸­æŠ½å–æ•°å€¼å‹featureï¼Œå¯¹åº”çš„åŠŸèƒ½åä¸ºï¼š</p>
<ul>
<li><strong>tokenizing</strong>ï¼šå¯¹stringsè¿›è¡ŒtokenåŒ–ï¼Œç»™æ¯ä¸ªå¯èƒ½çš„tokenä¸€ä¸ªinteger idï¼Œä¾‹å¦‚ï¼šä½¿ç”¨ç©ºæ ¼æˆ–é€—è±†åšä½œtokenåˆ†å‰²ç¬¦ã€‚</li>
<li><strong>counting</strong>: ç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­tokençš„å‡ºç°æ¬¡æ•°ã€‚</li>
<li><strong>normalizingå’Œweighting</strong>ï¼šå¯¹åœ¨å¤§å¤šæ•°æ ·æœ¬/æ–‡æ¡£ä¸­å‡ºç°çš„é‡è¦çš„tokenè¿›è¡Œå½’ä¸€åŒ–å’Œé™æƒã€‚</li>
</ul>
<p>åœ¨è¿™ç§schemeä¸‹ï¼Œfeatureså’Œsamplesçš„å®šä¹‰å¦‚ä¸‹:</p>
<ul>
<li>æ¯ä¸ªç‹¬ç«‹çš„tokenå‡ºç°ç‡ï¼ˆä¸ç®¡æ˜¯å¦å½’ä¸€åŒ–ï¼‰ï¼Œéƒ½è¢«å½“æˆä¸€ä¸ªfeatureã€‚</li>
<li>å¯¹äºä¸€ä¸ªç»™å®šçš„æ–‡æ¡£ï¼Œæ‰€æœ‰çš„tokené¢‘ç‡çš„å‘é‡éƒ½è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ª<strong>å¤šå…ƒæ ·æœ¬ï¼ˆmultivariate sampleï¼‰</strong>ã€‚</li>
</ul>
<p>ä¸€ä¸ªæ–‡æ¡£å‹è¯­æ–™å¯ä»¥è¢«è¡¨ç¤ºæˆä¸€ä¸ªçŸ©é˜µï¼šæ¯ä¸ªæ–‡æ¡£ä¸€è¡Œï¼Œæ¯ä¸ªåœ¨è¯­æ–™ä¸­å‡ºç°çš„tokenä¸€åˆ—ï¼ˆwordï¼‰ã€‚</p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä¼šæ¥æ”¶åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå®ƒåªèƒ½å¤„ç†æ•°å€¼å¼ é‡ã€‚æ–‡æœ¬å‘é‡åŒ–ï¼ˆvectorizeï¼‰æ˜¯æŒ‡å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å¼ é‡çš„è¿‡ç¨‹ã€‚ä¸»è¦æœ‰ä¸‰ç§å®ç°æ–¹å¼ï¼š1.å•è¯çº§ï¼›2å­—ç¬¦çº§ï¼›3.æå–å•è¯æˆ–è€…å­—ç¬¦çš„n-gram(å¤šä¸ªè¿ç»­çš„é›†åˆï¼‰<br>
å°†æ–‡æœ¬åˆ†è§£æˆçš„å•å…ƒï¼ˆå•è¯ã€å­—ç¬¦æˆ– n-gramï¼‰å«ä½œæ ‡è®°ï¼ˆtokenï¼‰ï¼Œå°†æ–‡æœ¬åˆ†è§£æˆæ ‡è®°çš„è¿‡ç¨‹å«ä½œåˆ†è¯ï¼ˆtokenizationï¼‰ã€‚æ‰€æœ‰æ–‡æœ¬å‘é‡åŒ–è¿‡ç¨‹éƒ½æ˜¯åº”ç”¨æŸç§åˆ†è¯æ–¹æ¡ˆï¼Œç„¶åå°†æ•°å€¼å‘é‡ä¸ç”Ÿæˆçš„æ ‡è®°ç›¸å…³è”ã€‚è¿™äº›å‘é‡ç»„åˆæˆåºåˆ—å¼ é‡ï¼Œè¢«è¾“å…¥åˆ°æ·±åº¦ç¥ç»ç½‘ç»œä¸­ã€‚å°†å‘é‡ä¸æ ‡è®°ç›¸å…³è”çš„ä¸»è¦æ–¹æ³•æœ‰ä¸¤ç§ï¼šåš one-hot ç¼–ç ï¼ˆone-hot encodingï¼‰ä¸æ ‡è®°åµŒå…¥ï¼»token embeddingï¼Œé€šå¸¸åªç”¨äºå•è¯ï¼Œå«ä½œè¯åµŒå…¥ï¼ˆword embeddingï¼‰ï¼½ã€‚</p>
<h5 id="embedding-å±‚">Embedding å±‚</h5>
<p><u>å°† Embedding å±‚ç†è§£ä¸ºä¸€ä¸ªå­—å…¸ï¼Œèƒ½å°†å•è¯ç´¢å¼•æ˜ å°„ä¸ºå¯†é›†å‘é‡ã€‚</u>å®ƒæ¥æ”¶æ•´æ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨å†…éƒ¨å­—å…¸ä¸­æŸ¥æ‰¾è¿™äº›æ•´æ•°ï¼Œç„¶åè¿”å›ç›¸å…³è”çš„å‘é‡ã€‚Embedding å±‚å®é™…ä¸Šæ˜¯ä¸€ç§å­—å…¸æŸ¥æ‰¾ã€‚</p>
<pre><code class="language-python">#  æ ‡è®°çš„ä¸ªæ•°ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼Œè¿™é‡Œæ˜¯ 1000ï¼Œå³æœ€å¤§å•è¯ç´¢å¼• +1ï¼‰å’ŒåµŒå…¥çš„ç»´åº¦ï¼ˆè¿™é‡Œæ˜¯ 64,æœ‰64ä¸ªç‰¹å¾ç»„æˆä¸€ä¸ªå•è¯ï¼‰
from keras.layers improt Embedding
embedding_layer = Embedding(1000,64)

</code></pre>
<h5 id="embedding-å±‚çš„è¾“å…¥å’Œè¾“å‡º">Embedding å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼š</h5>
<p>è¾“å…¥ï¼šä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸ºï¼ˆsamplesï¼Œsequential_lengthï¼‰<br>
samplesï¼šä»£è¡¨ä¸åŒçš„å¥å­ã€‚<br>
sequential_lengthï¼šä»£è¡¨å¥å­ä¸­çš„å•è¯çš„ä¸ªæ•°ï¼Œæ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªæ•°å­—ï¼Œä¸€å…±sequential_lengthä¸ªå•è¯ã€‚<br>
ä¸€æ‰¹æ•°æ®ä¸­çš„æ‰€æœ‰åºåˆ—å¿…é¡»å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼ˆå› ä¸ºéœ€è¦å°†å®ƒä»¬æ‰“åŒ…æˆä¸€ä¸ªå¼ é‡ï¼‰ï¼ŒçŸ­çš„è¡¥0ï¼Œé•¿çš„æˆªæ–­<br>
è¾“å‡ºï¼šä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸ºï¼ˆsamplesï¼Œsequential_lengthï¼Œdimensionalityï¼‰<br>
samplesï¼šä»£è¡¨ä¸åŒçš„å¥å­ã€‚<br>
sequential_lengthï¼šä»£è¡¨å¥å­ä¸­çš„å•è¯çš„ä¸ªæ•°<br>
dimensionalityï¼šä»£è¡¨é€šé“æ•°ï¼ŒåŒä¸€samplesï¼ŒåŒä¸€sequential_lengthä¸Šçš„æ‰€æœ‰é€šé“ä¸Šçš„å€¼ç»„æˆçš„å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯ï¼Œå¦‚ï¼ˆ0ï¼Œ0ï¼Œï¼šï¼‰ä»£è¡¨ä¸€ä¸ªå•è¯ã€‚</p>
<h5 id="embedding-å±‚çš„å­¦ä¹ å’Œç›®çš„">Embedding å±‚çš„å­¦ä¹ å’Œç›®çš„ï¼š</h5>
<p>å°†ä¸€ä¸ª Embedding å±‚å®ä¾‹åŒ–æ—¶ï¼Œå®ƒçš„æƒé‡ï¼ˆå³æ ‡è®°å‘é‡çš„å†…éƒ¨å­—å…¸ï¼‰æœ€å¼€å§‹æ˜¯éšæœºçš„ï¼Œä¸å…¶ä»–å±‚ä¸€æ ·ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åå‘ä¼ æ’­æ¥é€æ¸è°ƒèŠ‚è¿™äº›è¯å‘é‡ï¼Œæ”¹å˜ç©ºé—´ç»“æ„ä»¥ä¾¿ä¸‹æ¸¸æ¨¡å‹å¯ä»¥åˆ©ç”¨ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒåµŒå…¥ç©ºé—´å°†ä¼šå±•ç¤ºå¤§é‡ç»“æ„ï¼Œè¿™ç§ç»“æ„ä¸“é—¨é’ˆå¯¹è®­ç»ƒæ¨¡å‹æ‰€è¦è§£å†³çš„é—®é¢˜ã€‚</p>
<h5 id="embedding-å±‚çš„å®è·µ">Embedding å±‚çš„å®è·µï¼š</h5>
<p>åœºæ™¯ï¼šIMDB ç”µå½±è¯„è®ºæƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡<br>
æ–¹æ³•ï¼šå°†ç”µå½±è¯„è®ºé™åˆ¶ä¸ºå‰ 10 000 ä¸ªæœ€å¸¸è§çš„å•è¯ï¼ˆç¬¬ä¸€æ¬¡å¤„ç†è¿™ä¸ªæ•°æ®é›†æ—¶å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰ï¼Œç„¶åå°†è¯„è®ºé•¿åº¦é™åˆ¶ä¸ºåªæœ‰ 20 ä¸ªå•è¯ã€‚å¯¹äºè¿™ 10 000 ä¸ªå•è¯ï¼Œç½‘ç»œå°†å¯¹æ¯ä¸ªè¯éƒ½å­¦ä¹ ä¸€ä¸ª 8ç»´åµŒå…¥ï¼Œå°†è¾“å…¥çš„æ•´æ•°åºåˆ—ï¼ˆäºŒç»´æ•´æ•°å¼ é‡ï¼‰è½¬æ¢ä¸ºåµŒå…¥åºåˆ—ï¼ˆä¸‰ç»´æµ®ç‚¹æ•°å¼ é‡ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªå¼ é‡å±•å¹³ä¸ºäºŒç»´ï¼Œæœ€ååœ¨ä¸Šé¢è®­ç»ƒä¸€ä¸ª Dense å±‚ç”¨äºåˆ†ç±»</p>
<p>keras ç‰ˆæœ¬ï¼š</p>
<pre><code class="language-python">from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding
 
&quot;&quot;&quot;1.è·å–æ•°æ®ä½œä¸ºembeddingå±‚çš„è¾“å…¥ï¼šå°†æ•´æ•°åˆ—è¡¨è½¬æ¢æˆå½¢çŠ¶ä¸ºï¼ˆsamples,maxlenï¼‰çš„äºŒç»´æ•´æ•°å¼ é‡&quot;&quot;&quot;
max_features =10000 #å‰ä¸€ä¸‡ä¸ªå•è¯ç»„æˆè¯æ±‡è¡¨ï¼Œä¹Ÿå°±æ˜¯ont-hoté‡Œé¢ç‰¹å¾æ•°
maxlen=20#è¾“å…¥çš„å•è¯åºåˆ—é•¿åº¦è¦ç›¸åŒ
 
&quot;&quot;&quot;å°†æ•°æ®åŠ è½½ä¸ºæ•´æ•°åˆ—è¡¨&quot;&quot;&quot;
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
&quot;&quot;&quot;è¾“å…¥å‰è¦æˆªæˆç›¸åŒé•¿åº¦&quot;&quot;&quot;
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 
&quot;&quot;&quot;2.ä½¿ç”¨Embedding å±‚å’Œåˆ†ç±»å™¨&quot;&quot;&quot;
model=Sequential()
model.add(Embedding(10000,8,input_length=maxlen))
&quot;&quot;&quot;å°†ä¸‰ç»´çš„åµŒå…¥å¼ é‡å±•å¹³æˆå½¢çŠ¶ä¸º (samples, maxlen * 8) çš„äºŒç»´å¼ é‡ï¼š
ä¸€ä¸ªsampleçš„æ‰€æœ‰(maxlenä¸ª)å•è¯æ‰€æœ‰ï¼ˆ8ä¸ªï¼‰ç‰¹å¾,è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦æˆªå–ä¸€æ ·çš„é•¿åº¦&quot;&quot;&quot;
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])
model.summary()#è¾“å‡ºç½‘ç»œç»“æ„
#è®­ç»ƒæ¨¡å‹ï¼šè‡ªå·±åˆ‡åˆ†æ¥è®­ç»ƒ
history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)
</code></pre>
<p>tensorflow ç‰ˆæœ¬ï¼š</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

assert tf.__version__ &gt;= &quot;2.0&quot;

from pathlib import Path
from collections import Counter

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# åŠ è½½æ•°æ®é›†
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
path = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/datasets/aclImdb'



# glob.glob()ï¼šè¿”å›ç¬¦åˆåŒ¹é…æ¡ä»¶çš„æ‰€æœ‰æ–‡ä»¶çš„è·¯å¾„ï¼›
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]

# åˆå§‹åŒ–strä¸ºè·¯å¾„ å¦åˆ™æŠ¥é”™ï¼šAttributeError: 'str' object has no attribute 'glob'
path = Path(path)

train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

# å°†æµ‹è¯•é›†æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# TextLineDatasetè¯»å–csvæ‹†åˆ†ä¸ºnegå’Œposæ ‡ç­¾
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)


batch_size = 32
# å¯¹äºimdb_datasetæ“ä½œéœ€è¦cacheï¼ŒTextLineDatasetæ‰èƒ½è·å–å’Œä¹‹å‰openå‡½æ•°ä¸€æ ·çš„æ€§èƒ½
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# è¾“å…¥å‰è¦æˆªå–æˆç›¸åŒç¨‹åº¦çš„æ–‡æœ¬padçš„ä½œç”¨ä¹Ÿæ˜¯ä¿æŒæ¯ä¸ªtokençš„å•ä½é•¿åº¦
def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)


# ç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­tokençš„å‡ºç°æ¬¡æ•°å‡½æ•°
def get_vocabulary(data_sample, max_size=1000):
    preprocessed_reviews = preprocess(data_sample).numpy()
    counter = Counter()
    for words in preprocessed_reviews:
        for word in words:
            if word != b&quot;&lt;pad&gt;&quot;:
                counter[word] += 1
    return [b&quot;&lt;pad&gt;&quot;] + [word for word, count in counter.most_common(max_size)]


get_vocabulary(X_example)


# 1ã€preprocess ç”Ÿæˆ token
# 2ã€get_vocabulary ç»Ÿè®¡é¢‘ç‡
# å°†tokenç»Ÿè®¡é¢‘ç‡åçš„å½¢æˆtableä¸indxæ¥æ”¶æ•´æ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨å†…éƒ¨å­—å…¸ä¸­æŸ¥æ‰¾è¿™äº›æ•´æ•°ï¼Œç„¶åè¿”å›ç›¸å…³è”çš„å‘é‡
class TextVectorization(keras.layers.Layer):
    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.max_vocabulary_size = max_vocabulary_size
        self.n_oov_buckets = n_oov_buckets

    def adapt(self, data_sample):
        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)
        words = tf.constant(self.vocab)
        word_ids = tf.range(len(self.vocab), dtype=tf.int64)
        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)

    def call(self, inputs):
        preprocessed_inputs = preprocess(inputs)
        return self.table.lookup(preprocessed_inputs)


text_vectorization = TextVectorization()

text_vectorization.adapt(X_example)
text_vectorization(X_example)

max_vocabulary_size = 1000
n_oov_buckets = 100

sample_review_batches = train_set.map(lambda review, label: review)
sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),
                                axis=0)

text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,
                                       input_shape=[])
text_vectorization.adapt(sample_reviews)
text_vectorization(X_example)

simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])
tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)

print('text_vectorization.vocab: ', text_vectorization.vocab[:10])


class BagOfWords(keras.layers.Layer):
    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.n_tokens = n_tokens

    def call(self, inputs):
        one_hot = tf.one_hot(inputs, self.n_tokens)
        return tf.reduce_sum(one_hot, axis=1)[:, 1:]


bag_of_words = BagOfWords(n_tokens=4)
print('bag_of_words(simple_example): ', bag_of_words(simple_example))

n_tokens = max_vocabulary_size + n_oov_buckets + 1  # add 1 for &lt;pad&gt;
bag_of_words = BagOfWords(n_tokens)

model = keras.models.Sequential([
    text_vectorization,
    bag_of_words,
    keras.layers.Dense(100, activation=&quot;relu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;),
])
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])
model.fit(train_set, epochs=5, validation_data=valid_set)


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00001]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00001/">
        </link>
        <updated>2021-05-08T01:31:11.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>Taskï¼šå¯¹æ•°æ®é›†è¿›è¡Œæ‹†åˆ†ï¼Œåˆ›å»ºä¸€ä¸ªtf.data.Datasetæ¥åŠ è½½å¹¶è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶ååˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªåŒ…å«Embeddingå±‚çš„äºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹</p>
</blockquote>
<p>1.ä¸‹è½½å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†ï¼›</p>
<p>2.å°†æµ‹è¯•é›†æ‹†åˆ†ä¸ºä¸€ä¸ªéªŒè¯é›†å’Œä¸€ä¸ªæµ‹è¯•é›†</p>
<p>3.åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨TextVectorizationå±‚å¯¹æ¯ä¸ªè¯„è®ºè¿›è¡Œé¢„å¤„ç†ã€‚å¦‚æœTextVectorizationå±‚å°šä¸å¯ç”¨ï¼Œè¯·è‡ªå®šä¹‰é¢„å¤„ç†å±‚ã€‚</p>
<pre><code># TensorFlow â‰¥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

from IPython import get_ipython

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# åŠ è½½æ•°æ®é›†
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# ## 10.
# _Exercise: In this exercise you will download a dataset, split it,
# create a `tf.data.Dataset` to load it and preprocess it efficiently,
# then build and train a binary classification model containing an `Embedding` layer._
#
# ### a.
# _Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb),
# which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/).
# The data is organized in two directories, `train` and `test`,
# each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews.
# Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words),
# but we will ignore them in this exercise._




from pathlib import Path

# ä½¿ç”¨keras.utils.get_fileä¸‹è½½imdbæ•°æ®é›†ï¼Œcache_diræŒ‡å®šè‡ªå·±çš„æ–‡ä»¶ç›®å½•ï¼›
DOWNLOAD_ROOT = &quot;http://ai.stanford.edu/~amaas/data/sentiment/&quot;
FILENAME = &quot;aclImdb_v1.tar.gz&quot;
filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True,cache_dir=input_dir)
path = Path(filepath).parent / &quot;aclImdb&quot;
print(path)
# C:\Users\ericf\PycharmProjects\data_scientenist\data\raw\datasets\aclImdb

# æ‰“å°æ•°æ®é›†çš„å±‚çº§å…³ç³»å’Œæ–‡ä»¶å
#  os.walkæ ¹ç›®å½•ä¸‹çš„æ¯ä¸€ä¸ªæ–‡ä»¶å¤¹(åŒ…å«å®ƒè‡ªå·±), äº§ç”Ÿ3-å…ƒç»„ (dirpath, dirnames, filenames)ã€æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶å¤¹åå­—, æ–‡ä»¶åã€‘
#  name, subdirs, files=ã€æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶å¤¹åå­—, æ–‡ä»¶åã€‘
#  indent æ ¹æ®æ–‡ä»¶å¤¹ç›®å½•è®°å½•ç¼©è¿›
for name, subdirs, files in os.walk(path):
    indent = len(Path(name).parts) - len(path.parts)
    print(&quot;    &quot; * indent + Path(name).parts[-1] + os.sep)
    for index, filename in enumerate(sorted(files)):
        if index == 3:
            print(&quot;    &quot; * (indent + 1) + &quot;...&quot;)
            break
        print(&quot;    &quot; * (indent + 1) + filename)

#glob.glob()ï¼šè¿”å›ç¬¦åˆåŒ¹é…æ¡ä»¶çš„æ‰€æœ‰æ–‡ä»¶çš„è·¯å¾„ï¼›
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]


train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

print(len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg))

# ### b.
# _Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._
# å°†æµ‹è¯•é›†æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# ### c.
# _Exercise: Use tf.data to create an efficient dataset for each set._

# Since the dataset fits in memory,
# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:

# å¯¹äºå½±è¯„è¿›è¡ŒäºŒåˆ†ç±»æ ‡ç­¾å¤„ç†
def imdb_dataset(filepaths_positive, filepaths_negative):
    reviews = []
    labels = []
    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):
        for filepath in filepaths:
            with open(filepath,encoding='utf-8') as review_file:
                reviews.append(review_file.read())
            labels.append(label)
    return tf.data.Dataset.from_tensor_slices(
        (tf.constant(reviews), tf.constant(labels)))



for X, y in imdb_dataset(train_pos, train_neg).take(3):
    print(X)
    print(y)
    print()

#get_ipython().run_line_magic('timeit', '-r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass')


# It takes about 17 seconds to load the dataset and go through it 10 times.

# But let's pretend the dataset does not fit in memory, just to make things more interesting.
# Luckily, each review fits on just one line (they use `&lt;br /&gt;` to indicate line breaks),
# so we can read the reviews using a `TextLineDataset`.
# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).
# For very large datasets, it would make sense to use a tool like Apache Beam for that.


# 1.tf.data.TextLineDataset è¯»å–æ•°æ®é›†ï¼Œè‡ªåŠ¨åˆ›å»ºdataset
# 2.è¯»å–ä¹‹åä½¿ç”¨mapæ ¹æ®poså’Œnegè¿›è¡Œæ‰“æ ‡ç­¾æ“ä½œ
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)




# Now it takes about 33 seconds to go through the dataset 10 times. That's much slower,
# essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.
# If you add `.cache()` just before `.repeat(10)`,
# you will see that this implementation will be about as fast as the previous one.



batch_size = 32
# å¯¹äºimdb_datasetæ“ä½œéœ€è¦cacheï¼ŒTextLineDatasetæ‰èƒ½è·å–å’Œä¹‹å‰openå‡½æ•°ä¸€æ ·çš„æ€§èƒ½
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# ### d.
# _Exercise: Create a binary classification model,
# using a `TextVectorization` layer to preprocess each review.
# If the `TextVectorization` layer is not yet available (or if you like a challenge),
# try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package,
# for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces,
# and `split()` to split words on spaces.
# You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._

# Let's first write a function to preprocess the reviews, cropping them to 300 characters,
# converting them to lower case, then replacing `&lt;br /&gt;` and all non-letter characters to spaces,
# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:




def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSK_999]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_-xue-_001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">
        </link>
        <updated>2021-05-07T08:33:11.000Z</updated>
        <content type="html"><![CDATA[<p>#Adaboostã€GBDTã€XGBoostçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<p>Gradient Boosting å’Œå…¶å®ƒ Boosting ç®—æ³•ä¸€æ ·ï¼Œé€šè¿‡å°†è¡¨ç°ä¸€èˆ¬çš„æ•°ä¸ªæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯æ·±åº¦å›ºå®šçš„å†³ç­–æ ‘ï¼‰ç»„åˆåœ¨ä¸€èµ·æ¥é›†æˆä¸€ä¸ªè¡¨ç°è¾ƒå¥½çš„æ¨¡å‹ã€‚æŠ½è±¡åœ°è¯´ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ˜¯å¯¹ä¸€ä»»æ„å¯å¯¼ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡åå¤åœ°é€‰æ‹©ä¸€ä¸ªæŒ‡å‘è´Ÿæ¢¯åº¦æ–¹å‘çš„å‡½æ•°ï¼Œè¯¥ç®—æ³•å¯è¢«çœ‹åšåœ¨å‡½æ•°ç©ºé—´é‡Œå¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤å¯ä»¥è¯´ Gradient Boosting = Gradient Descent + Boostingã€‚</p>
<p>å’Œ AdaBoost ä¸€æ ·ï¼ŒGradient Boosting ä¹Ÿæ˜¯é‡å¤é€‰æ‹©ä¸€ä¸ªè¡¨ç°ä¸€èˆ¬çš„æ¨¡å‹å¹¶ä¸”æ¯æ¬¡åŸºäºå…ˆå‰æ¨¡å‹çš„è¡¨ç°è¿›è¡Œè°ƒæ•´ã€‚ä¸åŒçš„æ˜¯ï¼ŒAdaBoost æ˜¯é€šè¿‡æå‡é”™åˆ†æ•°æ®ç‚¹çš„æƒé‡æ¥å®šä½æ¨¡å‹çš„ä¸è¶³è€Œ Gradient Boosting æ˜¯é€šè¿‡ç®—æ¢¯åº¦ï¼ˆgradientï¼‰æ¥å®šä½æ¨¡å‹çš„ä¸è¶³ã€‚å› æ­¤ç›¸æ¯” AdaBoost, Gradient Boosting å¯ä»¥ä½¿ç”¨æ›´å¤šç§ç±»çš„ç›®æ ‡å‡½æ•°ã€‚</p>
<p><strong>Adaboost VS GBDTæœ€ä¸»è¦çš„åŒºåˆ«åœ¨äºä¸¤è€…å¦‚ä½•è¯†åˆ«æ¨¡å‹çš„é—®é¢˜</strong>ã€‚AdaBoostç”¨é”™åˆ†æ•°æ®ç‚¹æ¥è¯†åˆ«é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´é”™åˆ†æ•°æ®ç‚¹çš„æƒé‡æ¥æ”¹è¿›æ¨¡å‹ã€‚Gradient Boostingé€šè¿‡è´Ÿæ¢¯åº¦æ¥è¯†åˆ«é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—è´Ÿæ¢¯åº¦æ¥æ”¹è¿›æ¨¡å‹ã€‚</p>
<p>#GBDT ä¸ XGBoost åŒºåˆ«<br>
ä¼ ç»ŸGBDTä»¥CARTä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œxgboostè¿˜æ”¯æŒçº¿æ€§åˆ†ç±»å™¨ï¼Œè¿™ä¸ªæ—¶å€™xgboostç›¸å½“äºå¸¦L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„é€»è¾‘æ–¯è’‚å›å½’ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰æˆ–è€…çº¿æ€§å›å½’ï¼ˆå›å½’é—®é¢˜ï¼‰ã€‚<br>
ä¼ ç»ŸGBDTåœ¨ä¼˜åŒ–æ—¶åªç”¨åˆ°ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼Œxgbooståˆ™å¯¹ä»£ä»·å‡½æ•°è¿›è¡Œäº†äºŒé˜¶æ³°å‹’å±•å¼€ï¼ŒåŒæ—¶ç”¨åˆ°äº†ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ã€‚</p>
<p>xgbooståœ¨ä»£ä»·å‡½æ•°é‡ŒåŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€æ¯ä¸ªå¶å­èŠ‚ç‚¹ä¸Šè¾“å‡ºçš„scoreçš„L2æ¨¡çš„å¹³æ–¹å’Œã€‚ä»Bias-variance tradeoffè§’åº¦æ¥è®²ï¼Œæ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„varianceï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè¿™ä¹Ÿæ˜¯xgboostä¼˜äºä¼ ç»ŸGBDTçš„ä¸€ä¸ªç‰¹æ€§ã€‚</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://fa7ria.GitHub.io/hello-gridea/</id>
        <link href="https://fa7ria.GitHub.io/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>