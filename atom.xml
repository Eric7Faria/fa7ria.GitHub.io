<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://fa7ria.GitHub.io</id>
    <title>Fa7riaBlog</title>
    <updated>2021-05-12T06:33:38.806Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://fa7ria.GitHub.io"/>
    <link rel="self" href="https://fa7ria.GitHub.io/atom.xml"/>
    <subtitle>为学日益，为道日损。损之又损，以至于无为，无为而无不为</subtitle>
    <logo>https://fa7ria.GitHub.io/images/avatar.png</logo>
    <icon>https://fa7ria.GitHub.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Fa7riaBlog</rights>
    <entry>
        <title type="html"><![CDATA[DSP_00003_TFRecords]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00003_tfrecords/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00003_tfrecords/">
        </link>
        <updated>2021-05-11T12:57:03.000Z</updated>
        <content type="html"><![CDATA[<p>import tensorflow_datasets as tfds</p>
<p>datasets = tfds.load(name=&quot;mnist&quot;)<br>
mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;]<br>
print(tfds.list_builders())</p>
<h1 id="加载fashion-mnist数据集将其拆分为训练集-验证集-测试集">加载Fashion MNIST数据集；将其拆分为训练集、验证集、测试集；</h1>
<p>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()<br>
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]<br>
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]</p>
<p>keras.backend.clear_session()<br>
np.random.seed(7)<br>
tf.random.set_seed(77)</p>
<h1 id="shuffle打乱测试集-训练集-验证集的顺序">shuffle打乱测试集、训练集、验证集的顺序</h1>
<p>train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))<br>
valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))<br>
test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))</p>
<p>def create_example(image, label):<br>
image_data = tf.io.serialize_tensor(image)<br>
#image_data = tf.io.encode_jpeg(image[..., np.newaxis])<br>
return Example(<br>
features=Features(<br>
feature={<br>
&quot;image&quot;: Feature(bytes_list=BytesList(value=[image_data.numpy()])),<br>
&quot;label&quot;: Feature(int64_list=Int64List(value=[label])),<br>
}))</p>
<p>for image, label in valid_set.take(1):<br>
print(create_example(image, label))</p>
<p>from contextlib import ExitStack</p>
<h1 id=""></h1>
<p>def write_tfrecords(name, dataset, n_shards=10):<br>
paths = [&quot;{}.tfrecord-{:05d}-of-{:05d}&quot;.format(name, index, n_shards)<br>
for index in range(n_shards)]<br>
with ExitStack() as stack:<br>
writers = [stack.enter_context(tf.io.TFRecordWriter(path))<br>
for path in paths]<br>
for index, (image, label) in dataset.enumerate():<br>
shard = index % n_shards<br>
example = create_example(image, label)<br>
writers[shard].write(example.SerializeToString())<br>
return paths</p>
<p>train_filepaths = write_tfrecords(&quot;my_fashion_mnist.train&quot;, train_set)<br>
valid_filepaths = write_tfrecords(&quot;my_fashion_mnist.valid&quot;, valid_set)<br>
test_filepaths = write_tfrecords(&quot;my_fashion_mnist.test&quot;, test_set)</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00002_TextVectorization]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/">
        </link>
        <updated>2021-05-09T10:11:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>如果类别小于10，通常采用独热编码方式。如果类别大于50，通常使用嵌入编码。在10到50中间你需要实践比对一番，那种方式更好。</p>
</blockquote>
<p>我们将文本文档转成数值型feature向量的过程称为<strong>向量化（vectorization）</strong>。这种特定的策略（tokenization/counting/normalization）被称为词袋（Bag of Words）或者”Bag of n-grams”表示。通过单词出现率文档描述的文档会完全忽略文档中单词的相对位置。</p>
<h5 id="词嵌入的作用对于原始数据一串符号不能直接传给算法必须将它们表示成使用固定size的数值型的feature向量而非变长的原始文档">词嵌入的作用:对于原始数据，一串符号不能直接传给算法，必须将它们表示成使用固定size的数值型的feature向量，而非变长的原始文档。</h5>
<p>1、使用独热编码分类特征</p>
<p>2、使用嵌入编码分类特征</p>
<p><strong>Feature extraction与Feature Selection是完全不同的：前者将专有数据（文本或图片）转换成机器学习中可用的数值型特征；后者则是选取特征上的技术</strong></p>
<p>为了这个表示，sklearn提供了相应的工具类来从文本内容中抽取数值型feature，对应的功能名为：</p>
<ul>
<li><strong>tokenizing</strong>：对strings进行token化，给每个可能的token一个integer id，例如：使用空格或逗豆做作token分割符。</li>
<li><strong>counting</strong>: 统计在每个文档中token的出现次数。</li>
<li><strong>normalizing和weighting</strong>：对在大多数样本/文档中出现的重要的token进行归一化和降权。</li>
</ul>
<p>在这种scheme下，features和samples的定义如下:</p>
<ul>
<li>每个独立的token出现率（不管是否归一化），都被当成一个feature。</li>
<li>对于一个给定的文档，所有的token频率的向量都被认为是一个<strong>多元样本（multivariate sample）</strong>。</li>
</ul>
<p>一个文档型语料可以被表示成一个矩阵：每个文档一行，每个在语料中出现的token一列（word）。</p>
<p>深度学习模型不会接收原始文本作为输入，它只能处理数值张量。文本向量化（vectorize）是指将文本转换为数值张量的过程。主要有三种实现方式：1.单词级；2字符级；3.提取单词或者字符的n-gram(多个连续的集合）<br>
将文本分解成的单元（单词、字符或 n-gram）叫作标记（token），将文本分解成标记的过程叫作分词（tokenization）。所有文本向量化过程都是应用某种分词方案，然后将数值向量与生成的标记相关联。这些向量组合成序列张量，被输入到深度神经网络中。将向量与标记相关联的主要方法有两种：做 one-hot 编码（one-hot encoding）与标记嵌入［token embedding，通常只用于单词，叫作词嵌入（word embedding）］。</p>
<h5 id="embedding-层">Embedding 层</h5>
<p><u>将 Embedding 层理解为一个字典，能将单词索引映射为密集向量。</u>它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding 层实际上是一种字典查找。</p>
<pre><code class="language-python">#  标记的个数（词汇表大小，这里是 1000，即最大单词索引 +1）和嵌入的维度（这里是 64,有64个特征组成一个单词）
from keras.layers improt Embedding
embedding_layer = Embedding(1000,64)

</code></pre>
<h5 id="embedding-层的输入和输出">Embedding 层的输入和输出：</h5>
<p>输入：一个二维张量，形状为（samples，sequential_length）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数，每个单词对应一个数字，一共sequential_length个单词。<br>
一批数据中的所有序列必须具有相同的长度（因为需要将它们打包成一个张量），短的补0，长的截断<br>
输出：一个三维张量，形状为（samples，sequential_length，dimensionality）<br>
samples：代表不同的句子。<br>
sequential_length：代表句子中的单词的个数<br>
dimensionality：代表通道数，同一samples，同一sequential_length上的所有通道上的值组成的向量表示一个单词，如（0，0，：）代表一个单词。</p>
<h5 id="embedding-层的学习和目的">Embedding 层的学习和目的：</h5>
<p>将一个 Embedding 层实例化时，它的权重（即标记向量的内部字典）最开始是随机的，与其他层一样。在训练过程中，利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以利用。一旦训练完成，嵌入空间将会展示大量结构，这种结构专门针对训练模型所要解决的问题。</p>
<h5 id="embedding-层的实践">Embedding 层的实践：</h5>
<p>场景：IMDB 电影评论情感预测任务<br>
方法：将电影评论限制为前 10 000 个最常见的单词（第一次处理这个数据集时就是这么做的），然后将评论长度限制为只有 20 个单词。对于这 10 000 个单词，网络将对每个词都学习一个 8维嵌入，将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量），然后将这个张量展平为二维，最后在上面训练一个 Dense 层用于分类</p>
<p>keras 版本：</p>
<pre><code class="language-python">from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding
 
&quot;&quot;&quot;1.获取数据作为embedding层的输入：将整数列表转换成形状为（samples,maxlen）的二维整数张量&quot;&quot;&quot;
max_features =10000 #前一万个单词组成词汇表，也就是ont-hot里面特征数
maxlen=20#输入的单词序列长度要相同
 
&quot;&quot;&quot;将数据加载为整数列表&quot;&quot;&quot;
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
&quot;&quot;&quot;输入前要截成相同长度&quot;&quot;&quot;
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 
&quot;&quot;&quot;2.使用Embedding 层和分类器&quot;&quot;&quot;
model=Sequential()
model.add(Embedding(10000,8,input_length=maxlen))
&quot;&quot;&quot;将三维的嵌入张量展平成形状为 (samples, maxlen * 8) 的二维张量：
一个sample的所有(maxlen个)单词所有（8个）特征,这就是为什么要截取一样的长度&quot;&quot;&quot;
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])
model.summary()#输出网络结构
#训练模型：自己切分来训练
history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)
</code></pre>
<p>tensorflow 版本：</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

assert tf.__version__ &gt;= &quot;2.0&quot;

from pathlib import Path
from collections import Counter

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
path = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/datasets/aclImdb'



# glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]

# 初始化str为路径 否则报错：AttributeError: 'str' object has no attribute 'glob'
path = Path(path)

train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# TextLineDataset读取csv拆分为neg和pos标签
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)


batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# 输入前要截取成相同程度的文本pad的作用也是保持每个token的单位长度
def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)


# 统计在每个文档中token的出现次数函数
def get_vocabulary(data_sample, max_size=1000):
    preprocessed_reviews = preprocess(data_sample).numpy()
    counter = Counter()
    for words in preprocessed_reviews:
        for word in words:
            if word != b&quot;&lt;pad&gt;&quot;:
                counter[word] += 1
    return [b&quot;&lt;pad&gt;&quot;] + [word for word, count in counter.most_common(max_size)]


get_vocabulary(X_example)


# 1、preprocess 生成 token
# 2、get_vocabulary 统计频率
# 将token统计频率后的形成table与indx接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量
class TextVectorization(keras.layers.Layer):
    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.max_vocabulary_size = max_vocabulary_size
        self.n_oov_buckets = n_oov_buckets

    def adapt(self, data_sample):
        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)
        words = tf.constant(self.vocab)
        word_ids = tf.range(len(self.vocab), dtype=tf.int64)
        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)

    def call(self, inputs):
        preprocessed_inputs = preprocess(inputs)
        return self.table.lookup(preprocessed_inputs)


text_vectorization = TextVectorization()

text_vectorization.adapt(X_example)
text_vectorization(X_example)

max_vocabulary_size = 1000
n_oov_buckets = 100

sample_review_batches = train_set.map(lambda review, label: review)
sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),
                                axis=0)

text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,
                                       input_shape=[])
text_vectorization.adapt(sample_reviews)
text_vectorization(X_example)

simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])
tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)

print('text_vectorization.vocab: ', text_vectorization.vocab[:10])


class BagOfWords(keras.layers.Layer):
    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.n_tokens = n_tokens

    def call(self, inputs):
        one_hot = tf.one_hot(inputs, self.n_tokens)
        return tf.reduce_sum(one_hot, axis=1)[:, 1:]


bag_of_words = BagOfWords(n_tokens=4)
print('bag_of_words(simple_example): ', bag_of_words(simple_example))

n_tokens = max_vocabulary_size + n_oov_buckets + 1  # add 1 for &lt;pad&gt;
bag_of_words = BagOfWords(n_tokens)

model = keras.models.Sequential([
    text_vectorization,
    bag_of_words,
    keras.layers.Dense(100, activation=&quot;relu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;),
])
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])
model.fit(train_set, epochs=5, validation_data=valid_set)


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00001]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00001/">
        </link>
        <updated>2021-05-08T01:31:11.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>Task：对数据集进行拆分，创建一个tf.data.Dataset来加载并进行预处理，然后创建和训练一个包含Embedding层的二进制分类模型</p>
</blockquote>
<p>1.下载大型电影评论数据集；</p>
<p>2.将测试集拆分为一个验证集和一个测试集</p>
<p>3.创建一个二进制分类模型，使用TextVectorization层对每个评论进行预处理。如果TextVectorization层尚不可用，请自定义预处理层。</p>
<pre><code># TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

from IPython import get_ipython

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# 加载数据集
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# ## 10.
# _Exercise: In this exercise you will download a dataset, split it,
# create a `tf.data.Dataset` to load it and preprocess it efficiently,
# then build and train a binary classification model containing an `Embedding` layer._
#
# ### a.
# _Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb),
# which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/).
# The data is organized in two directories, `train` and `test`,
# each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews.
# Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words),
# but we will ignore them in this exercise._




from pathlib import Path

# 使用keras.utils.get_file下载imdb数据集，cache_dir指定自己的文件目录；
DOWNLOAD_ROOT = &quot;http://ai.stanford.edu/~amaas/data/sentiment/&quot;
FILENAME = &quot;aclImdb_v1.tar.gz&quot;
filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True,cache_dir=input_dir)
path = Path(filepath).parent / &quot;aclImdb&quot;
print(path)
# C:\Users\ericf\PycharmProjects\data_scientenist\data\raw\datasets\aclImdb

# 打印数据集的层级关系和文件名
#  os.walk根目录下的每一个文件夹(包含它自己), 产生3-元组 (dirpath, dirnames, filenames)【文件夹路径, 文件夹名字, 文件名】
#  name, subdirs, files=【文件夹路径, 文件夹名字, 文件名】
#  indent 根据文件夹目录记录缩进
for name, subdirs, files in os.walk(path):
    indent = len(Path(name).parts) - len(path.parts)
    print(&quot;    &quot; * indent + Path(name).parts[-1] + os.sep)
    for index, filename in enumerate(sorted(files)):
        if index == 3:
            print(&quot;    &quot; * (indent + 1) + &quot;...&quot;)
            break
        print(&quot;    &quot; * (indent + 1) + filename)

#glob.glob()：返回符合匹配条件的所有文件的路径；
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]


train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

print(len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg))

# ### b.
# _Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._
# 将测试集拆分测试集和验证集
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# ### c.
# _Exercise: Use tf.data to create an efficient dataset for each set._

# Since the dataset fits in memory,
# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:

# 对于影评进行二分类标签处理
def imdb_dataset(filepaths_positive, filepaths_negative):
    reviews = []
    labels = []
    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):
        for filepath in filepaths:
            with open(filepath,encoding='utf-8') as review_file:
                reviews.append(review_file.read())
            labels.append(label)
    return tf.data.Dataset.from_tensor_slices(
        (tf.constant(reviews), tf.constant(labels)))



for X, y in imdb_dataset(train_pos, train_neg).take(3):
    print(X)
    print(y)
    print()

#get_ipython().run_line_magic('timeit', '-r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass')


# It takes about 17 seconds to load the dataset and go through it 10 times.

# But let's pretend the dataset does not fit in memory, just to make things more interesting.
# Luckily, each review fits on just one line (they use `&lt;br /&gt;` to indicate line breaks),
# so we can read the reviews using a `TextLineDataset`.
# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).
# For very large datasets, it would make sense to use a tool like Apache Beam for that.


# 1.tf.data.TextLineDataset 读取数据集，自动创建dataset
# 2.读取之后使用map根据pos和neg进行打标签操作
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)




# Now it takes about 33 seconds to go through the dataset 10 times. That's much slower,
# essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.
# If you add `.cache()` just before `.repeat(10)`,
# you will see that this implementation will be about as fast as the previous one.



batch_size = 32
# 对于imdb_dataset操作需要cache，TextLineDataset才能获取和之前open函数一样的性能
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# ### d.
# _Exercise: Create a binary classification model,
# using a `TextVectorization` layer to preprocess each review.
# If the `TextVectorization` layer is not yet available (or if you like a challenge),
# try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package,
# for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces,
# and `split()` to split words on spaces.
# You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._

# Let's first write a function to preprocess the reviews, cropping them to 300 characters,
# converting them to lower case, then replacing `&lt;br /&gt;` and all non-letter characters to spaces,
# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:




def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSK_999]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_-xue-_001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">
        </link>
        <updated>2021-05-07T08:33:11.000Z</updated>
        <content type="html"><![CDATA[<p>#Adaboost、GBDT、XGBoost的区别是什么？</p>
<p>Gradient Boosting 和其它 Boosting 算法一样，通过将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说 Gradient Boosting = Gradient Descent + Boosting。</p>
<p>和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost 是通过提升错分数据点的权重来定位模型的不足而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost, Gradient Boosting 可以使用更多种类的目标函数。</p>
<p><strong>Adaboost VS GBDT最主要的区别在于两者如何识别模型的问题</strong>。AdaBoost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。Gradient Boosting通过负梯度来识别问题，通过计算负梯度来改进模型。</p>
<p>#GBDT 与 XGBoost 区别<br>
传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。<br>
传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p>
<p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://fa7ria.GitHub.io/hello-gridea/</id>
        <link href="https://fa7ria.GitHub.io/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>