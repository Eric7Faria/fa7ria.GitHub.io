<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://fa7ria.GitHub.io</id>
    <title>Fa7riaBlog</title>
    <updated>2021-05-12T06:33:38.806Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://fa7ria.GitHub.io"/>
    <link rel="self" href="https://fa7ria.GitHub.io/atom.xml"/>
    <subtitle>ä¸ºå­¦æ—¥ç›Šï¼Œä¸ºé“æ—¥æŸã€‚æŸä¹‹åˆæŸï¼Œä»¥è‡³äºæ— ä¸ºï¼Œæ— ä¸ºè€Œæ— ä¸ä¸º</subtitle>
    <logo>https://fa7ria.GitHub.io/images/avatar.png</logo>
    <icon>https://fa7ria.GitHub.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Fa7riaBlog</rights>
    <entry>
        <title type="html"><![CDATA[DSP_00003_TFRecords]]></title>
        <id>https://fa7ria.GitHub.io/dsp_00003_tfrecords/</id>
        <link href="https://fa7ria.GitHub.io/dsp_00003_tfrecords/">
        </link>
        <updated>2021-05-11T12:57:03.000Z</updated>
        <content type="html"><![CDATA[<p>import tensorflow_datasets as tfds</p>
<p>datasets = tfds.load(name=&quot;mnist&quot;)<br>
mnist_train, mnist_test = datasets[&quot;train&quot;], datasets[&quot;test&quot;]<br>
print(tfds.list_builders())</p>
<h1 id="åŠ è½½fashion-mnistæ•°æ®é›†å°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›†">åŠ è½½Fashion MNISTæ•°æ®é›†ï¼›å°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼›</h1>
<p>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()<br>
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]<br>
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]</p>
<p>keras.backend.clear_session()<br>
np.random.seed(7)<br>
tf.random.set_seed(77)</p>
<h1 id="shuffleæ‰“ä¹±æµ‹è¯•é›†-è®­ç»ƒé›†-éªŒè¯é›†çš„é¡ºåº">shuffleæ‰“ä¹±æµ‹è¯•é›†ã€è®­ç»ƒé›†ã€éªŒè¯é›†çš„é¡ºåº</h1>
<p>train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))<br>
valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))<br>
test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))</p>
<p>def create_example(image, label):<br>
image_data = tf.io.serialize_tensor(image)<br>
#image_data = tf.io.encode_jpeg(image[..., np.newaxis])<br>
return Example(<br>
features=Features(<br>
feature={<br>
&quot;image&quot;: Feature(bytes_list=BytesList(value=[image_data.numpy()])),<br>
&quot;label&quot;: Feature(int64_list=Int64List(value=[label])),<br>
}))</p>
<p>for image, label in valid_set.take(1):<br>
print(create_example(image, label))</p>
<p>from contextlib import ExitStack</p>
<h1 id=""></h1>
<p>def write_tfrecords(name, dataset, n_shards=10):<br>
paths = [&quot;{}.tfrecord-{:05d}-of-{:05d}&quot;.format(name, index, n_shards)<br>
for index in range(n_shards)]<br>
with ExitStack() as stack:<br>
writers = [stack.enter_context(tf.io.TFRecordWriter(path))<br>
for path in paths]<br>
for index, (image, label) in dataset.enumerate():<br>
shard = index % n_shards<br>
example = create_example(image, label)<br>
writers[shard].write(example.SerializeToString())<br>
return paths</p>
<p>train_filepaths = write_tfrecords(&quot;my_fashion_mnist.train&quot;, train_set)<br>
valid_filepaths = write_tfrecords(&quot;my_fashion_mnist.valid&quot;, valid_set)<br>
test_filepaths = write_tfrecords(&quot;my_fashion_mnist.test&quot;, test_set)</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00002_TextVectorization]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00002_textvectorization/">
        </link>
        <updated>2021-05-09T10:11:27.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>å¦‚æœç±»åˆ«å°äº10ï¼Œé€šå¸¸é‡‡ç”¨ç‹¬çƒ­ç¼–ç æ–¹å¼ã€‚å¦‚æœç±»åˆ«å¤§äº50ï¼Œé€šå¸¸ä½¿ç”¨åµŒå…¥ç¼–ç ã€‚åœ¨10åˆ°50ä¸­é—´ä½ éœ€è¦å®è·µæ¯”å¯¹ä¸€ç•ªï¼Œé‚£ç§æ–¹å¼æ›´å¥½ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬å°†æ–‡æœ¬æ–‡æ¡£è½¬æˆæ•°å€¼å‹featureå‘é‡çš„è¿‡ç¨‹ç§°ä¸º<strong>å‘é‡åŒ–ï¼ˆvectorizationï¼‰</strong>ã€‚è¿™ç§ç‰¹å®šçš„ç­–ç•¥ï¼ˆtokenization/counting/normalizationï¼‰è¢«ç§°ä¸ºè¯è¢‹ï¼ˆBag of Wordsï¼‰æˆ–è€…â€Bag of n-gramsâ€è¡¨ç¤ºã€‚é€šè¿‡å•è¯å‡ºç°ç‡æ–‡æ¡£æè¿°çš„æ–‡æ¡£ä¼šå®Œå…¨å¿½ç•¥æ–‡æ¡£ä¸­å•è¯çš„ç›¸å¯¹ä½ç½®ã€‚</p>
<h5 id="è¯åµŒå…¥çš„ä½œç”¨å¯¹äºåŸå§‹æ•°æ®ä¸€ä¸²ç¬¦å·ä¸èƒ½ç›´æ¥ä¼ ç»™ç®—æ³•å¿…é¡»å°†å®ƒä»¬è¡¨ç¤ºæˆä½¿ç”¨å›ºå®šsizeçš„æ•°å€¼å‹çš„featureå‘é‡è€Œéå˜é•¿çš„åŸå§‹æ–‡æ¡£">è¯åµŒå…¥çš„ä½œç”¨:å¯¹äºåŸå§‹æ•°æ®ï¼Œä¸€ä¸²ç¬¦å·ä¸èƒ½ç›´æ¥ä¼ ç»™ç®—æ³•ï¼Œå¿…é¡»å°†å®ƒä»¬è¡¨ç¤ºæˆä½¿ç”¨å›ºå®šsizeçš„æ•°å€¼å‹çš„featureå‘é‡ï¼Œè€Œéå˜é•¿çš„åŸå§‹æ–‡æ¡£ã€‚</h5>
<p>1ã€ä½¿ç”¨ç‹¬çƒ­ç¼–ç åˆ†ç±»ç‰¹å¾</p>
<p>2ã€ä½¿ç”¨åµŒå…¥ç¼–ç åˆ†ç±»ç‰¹å¾</p>
<p><strong>Feature extractionä¸Feature Selectionæ˜¯å®Œå…¨ä¸åŒçš„ï¼šå‰è€…å°†ä¸“æœ‰æ•°æ®ï¼ˆæ–‡æœ¬æˆ–å›¾ç‰‡ï¼‰è½¬æ¢æˆæœºå™¨å­¦ä¹ ä¸­å¯ç”¨çš„æ•°å€¼å‹ç‰¹å¾ï¼›åè€…åˆ™æ˜¯é€‰å–ç‰¹å¾ä¸Šçš„æŠ€æœ¯</strong></p>
<p>ä¸ºäº†è¿™ä¸ªè¡¨ç¤ºï¼Œsklearnæä¾›äº†ç›¸åº”çš„å·¥å…·ç±»æ¥ä»æ–‡æœ¬å†…å®¹ä¸­æŠ½å–æ•°å€¼å‹featureï¼Œå¯¹åº”çš„åŠŸèƒ½åä¸ºï¼š</p>
<ul>
<li><strong>tokenizing</strong>ï¼šå¯¹stringsè¿›è¡ŒtokenåŒ–ï¼Œç»™æ¯ä¸ªå¯èƒ½çš„tokenä¸€ä¸ªinteger idï¼Œä¾‹å¦‚ï¼šä½¿ç”¨ç©ºæ ¼æˆ–é€—è±†åšä½œtokenåˆ†å‰²ç¬¦ã€‚</li>
<li><strong>counting</strong>: ç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­tokençš„å‡ºç°æ¬¡æ•°ã€‚</li>
<li><strong>normalizingå’Œweighting</strong>ï¼šå¯¹åœ¨å¤§å¤šæ•°æ ·æœ¬/æ–‡æ¡£ä¸­å‡ºç°çš„é‡è¦çš„tokenè¿›è¡Œå½’ä¸€åŒ–å’Œé™æƒã€‚</li>
</ul>
<p>åœ¨è¿™ç§schemeä¸‹ï¼Œfeatureså’Œsamplesçš„å®šä¹‰å¦‚ä¸‹:</p>
<ul>
<li>æ¯ä¸ªç‹¬ç«‹çš„tokenå‡ºç°ç‡ï¼ˆä¸ç®¡æ˜¯å¦å½’ä¸€åŒ–ï¼‰ï¼Œéƒ½è¢«å½“æˆä¸€ä¸ªfeatureã€‚</li>
<li>å¯¹äºä¸€ä¸ªç»™å®šçš„æ–‡æ¡£ï¼Œæ‰€æœ‰çš„tokené¢‘ç‡çš„å‘é‡éƒ½è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ª<strong>å¤šå…ƒæ ·æœ¬ï¼ˆmultivariate sampleï¼‰</strong>ã€‚</li>
</ul>
<p>ä¸€ä¸ªæ–‡æ¡£å‹è¯­æ–™å¯ä»¥è¢«è¡¨ç¤ºæˆä¸€ä¸ªçŸ©é˜µï¼šæ¯ä¸ªæ–‡æ¡£ä¸€è¡Œï¼Œæ¯ä¸ªåœ¨è¯­æ–™ä¸­å‡ºç°çš„tokenä¸€åˆ—ï¼ˆwordï¼‰ã€‚</p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä¼šæ¥æ”¶åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå®ƒåªèƒ½å¤„ç†æ•°å€¼å¼ é‡ã€‚æ–‡æœ¬å‘é‡åŒ–ï¼ˆvectorizeï¼‰æ˜¯æŒ‡å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å¼ é‡çš„è¿‡ç¨‹ã€‚ä¸»è¦æœ‰ä¸‰ç§å®ç°æ–¹å¼ï¼š1.å•è¯çº§ï¼›2å­—ç¬¦çº§ï¼›3.æå–å•è¯æˆ–è€…å­—ç¬¦çš„n-gram(å¤šä¸ªè¿ç»­çš„é›†åˆï¼‰<br>
å°†æ–‡æœ¬åˆ†è§£æˆçš„å•å…ƒï¼ˆå•è¯ã€å­—ç¬¦æˆ– n-gramï¼‰å«ä½œæ ‡è®°ï¼ˆtokenï¼‰ï¼Œå°†æ–‡æœ¬åˆ†è§£æˆæ ‡è®°çš„è¿‡ç¨‹å«ä½œåˆ†è¯ï¼ˆtokenizationï¼‰ã€‚æ‰€æœ‰æ–‡æœ¬å‘é‡åŒ–è¿‡ç¨‹éƒ½æ˜¯åº”ç”¨æŸç§åˆ†è¯æ–¹æ¡ˆï¼Œç„¶åå°†æ•°å€¼å‘é‡ä¸ç”Ÿæˆçš„æ ‡è®°ç›¸å…³è”ã€‚è¿™äº›å‘é‡ç»„åˆæˆåºåˆ—å¼ é‡ï¼Œè¢«è¾“å…¥åˆ°æ·±åº¦ç¥ç»ç½‘ç»œä¸­ã€‚å°†å‘é‡ä¸æ ‡è®°ç›¸å…³è”çš„ä¸»è¦æ–¹æ³•æœ‰ä¸¤ç§ï¼šåš one-hot ç¼–ç ï¼ˆone-hot encodingï¼‰ä¸æ ‡è®°åµŒå…¥ï¼»token embeddingï¼Œé€šå¸¸åªç”¨äºå•è¯ï¼Œå«ä½œè¯åµŒå…¥ï¼ˆword embeddingï¼‰ï¼½ã€‚</p>
<h5 id="embedding-å±‚">Embedding å±‚</h5>
<p><u>å°† Embedding å±‚ç†è§£ä¸ºä¸€ä¸ªå­—å…¸ï¼Œèƒ½å°†å•è¯ç´¢å¼•æ˜ å°„ä¸ºå¯†é›†å‘é‡ã€‚</u>å®ƒæ¥æ”¶æ•´æ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨å†…éƒ¨å­—å…¸ä¸­æŸ¥æ‰¾è¿™äº›æ•´æ•°ï¼Œç„¶åè¿”å›ç›¸å…³è”çš„å‘é‡ã€‚Embedding å±‚å®é™…ä¸Šæ˜¯ä¸€ç§å­—å…¸æŸ¥æ‰¾ã€‚</p>
<pre><code class="language-python">#  æ ‡è®°çš„ä¸ªæ•°ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼Œè¿™é‡Œæ˜¯ 1000ï¼Œå³æœ€å¤§å•è¯ç´¢å¼• +1ï¼‰å’ŒåµŒå…¥çš„ç»´åº¦ï¼ˆè¿™é‡Œæ˜¯ 64,æœ‰64ä¸ªç‰¹å¾ç»„æˆä¸€ä¸ªå•è¯ï¼‰
from keras.layers improt Embedding
embedding_layer = Embedding(1000,64)

</code></pre>
<h5 id="embedding-å±‚çš„è¾“å…¥å’Œè¾“å‡º">Embedding å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼š</h5>
<p>è¾“å…¥ï¼šä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸ºï¼ˆsamplesï¼Œsequential_lengthï¼‰<br>
samplesï¼šä»£è¡¨ä¸åŒçš„å¥å­ã€‚<br>
sequential_lengthï¼šä»£è¡¨å¥å­ä¸­çš„å•è¯çš„ä¸ªæ•°ï¼Œæ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªæ•°å­—ï¼Œä¸€å…±sequential_lengthä¸ªå•è¯ã€‚<br>
ä¸€æ‰¹æ•°æ®ä¸­çš„æ‰€æœ‰åºåˆ—å¿…é¡»å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼ˆå› ä¸ºéœ€è¦å°†å®ƒä»¬æ‰“åŒ…æˆä¸€ä¸ªå¼ é‡ï¼‰ï¼ŒçŸ­çš„è¡¥0ï¼Œé•¿çš„æˆªæ–­<br>
è¾“å‡ºï¼šä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸ºï¼ˆsamplesï¼Œsequential_lengthï¼Œdimensionalityï¼‰<br>
samplesï¼šä»£è¡¨ä¸åŒçš„å¥å­ã€‚<br>
sequential_lengthï¼šä»£è¡¨å¥å­ä¸­çš„å•è¯çš„ä¸ªæ•°<br>
dimensionalityï¼šä»£è¡¨é€šé“æ•°ï¼ŒåŒä¸€samplesï¼ŒåŒä¸€sequential_lengthä¸Šçš„æ‰€æœ‰é€šé“ä¸Šçš„å€¼ç»„æˆçš„å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯ï¼Œå¦‚ï¼ˆ0ï¼Œ0ï¼Œï¼šï¼‰ä»£è¡¨ä¸€ä¸ªå•è¯ã€‚</p>
<h5 id="embedding-å±‚çš„å­¦ä¹ å’Œç›®çš„">Embedding å±‚çš„å­¦ä¹ å’Œç›®çš„ï¼š</h5>
<p>å°†ä¸€ä¸ª Embedding å±‚å®ä¾‹åŒ–æ—¶ï¼Œå®ƒçš„æƒé‡ï¼ˆå³æ ‡è®°å‘é‡çš„å†…éƒ¨å­—å…¸ï¼‰æœ€å¼€å§‹æ˜¯éšæœºçš„ï¼Œä¸å…¶ä»–å±‚ä¸€æ ·ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åå‘ä¼ æ’­æ¥é€æ¸è°ƒèŠ‚è¿™äº›è¯å‘é‡ï¼Œæ”¹å˜ç©ºé—´ç»“æ„ä»¥ä¾¿ä¸‹æ¸¸æ¨¡å‹å¯ä»¥åˆ©ç”¨ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒåµŒå…¥ç©ºé—´å°†ä¼šå±•ç¤ºå¤§é‡ç»“æ„ï¼Œè¿™ç§ç»“æ„ä¸“é—¨é’ˆå¯¹è®­ç»ƒæ¨¡å‹æ‰€è¦è§£å†³çš„é—®é¢˜ã€‚</p>
<h5 id="embedding-å±‚çš„å®è·µ">Embedding å±‚çš„å®è·µï¼š</h5>
<p>åœºæ™¯ï¼šIMDB ç”µå½±è¯„è®ºæƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡<br>
æ–¹æ³•ï¼šå°†ç”µå½±è¯„è®ºé™åˆ¶ä¸ºå‰ 10 000 ä¸ªæœ€å¸¸è§çš„å•è¯ï¼ˆç¬¬ä¸€æ¬¡å¤„ç†è¿™ä¸ªæ•°æ®é›†æ—¶å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰ï¼Œç„¶åå°†è¯„è®ºé•¿åº¦é™åˆ¶ä¸ºåªæœ‰ 20 ä¸ªå•è¯ã€‚å¯¹äºè¿™ 10 000 ä¸ªå•è¯ï¼Œç½‘ç»œå°†å¯¹æ¯ä¸ªè¯éƒ½å­¦ä¹ ä¸€ä¸ª 8ç»´åµŒå…¥ï¼Œå°†è¾“å…¥çš„æ•´æ•°åºåˆ—ï¼ˆäºŒç»´æ•´æ•°å¼ é‡ï¼‰è½¬æ¢ä¸ºåµŒå…¥åºåˆ—ï¼ˆä¸‰ç»´æµ®ç‚¹æ•°å¼ é‡ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªå¼ é‡å±•å¹³ä¸ºäºŒç»´ï¼Œæœ€ååœ¨ä¸Šé¢è®­ç»ƒä¸€ä¸ª Dense å±‚ç”¨äºåˆ†ç±»</p>
<p>keras ç‰ˆæœ¬ï¼š</p>
<pre><code class="language-python">from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding
 
&quot;&quot;&quot;1.è·å–æ•°æ®ä½œä¸ºembeddingå±‚çš„è¾“å…¥ï¼šå°†æ•´æ•°åˆ—è¡¨è½¬æ¢æˆå½¢çŠ¶ä¸ºï¼ˆsamples,maxlenï¼‰çš„äºŒç»´æ•´æ•°å¼ é‡&quot;&quot;&quot;
max_features =10000 #å‰ä¸€ä¸‡ä¸ªå•è¯ç»„æˆè¯æ±‡è¡¨ï¼Œä¹Ÿå°±æ˜¯ont-hoté‡Œé¢ç‰¹å¾æ•°
maxlen=20#è¾“å…¥çš„å•è¯åºåˆ—é•¿åº¦è¦ç›¸åŒ
 
&quot;&quot;&quot;å°†æ•°æ®åŠ è½½ä¸ºæ•´æ•°åˆ—è¡¨&quot;&quot;&quot;
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
&quot;&quot;&quot;è¾“å…¥å‰è¦æˆªæˆç›¸åŒé•¿åº¦&quot;&quot;&quot;
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
 
&quot;&quot;&quot;2.ä½¿ç”¨Embedding å±‚å’Œåˆ†ç±»å™¨&quot;&quot;&quot;
model=Sequential()
model.add(Embedding(10000,8,input_length=maxlen))
&quot;&quot;&quot;å°†ä¸‰ç»´çš„åµŒå…¥å¼ é‡å±•å¹³æˆå½¢çŠ¶ä¸º (samples, maxlen * 8) çš„äºŒç»´å¼ é‡ï¼š
ä¸€ä¸ªsampleçš„æ‰€æœ‰(maxlenä¸ª)å•è¯æ‰€æœ‰ï¼ˆ8ä¸ªï¼‰ç‰¹å¾,è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦æˆªå–ä¸€æ ·çš„é•¿åº¦&quot;&quot;&quot;
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])
model.summary()#è¾“å‡ºç½‘ç»œç»“æ„
#è®­ç»ƒæ¨¡å‹ï¼šè‡ªå·±åˆ‡åˆ†æ¥è®­ç»ƒ
history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)
</code></pre>
<p>tensorflow ç‰ˆæœ¬ï¼š</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

assert tf.__version__ &gt;= &quot;2.0&quot;

from pathlib import Path
from collections import Counter

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# åŠ è½½æ•°æ®é›†
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
path = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/datasets/aclImdb'



# glob.glob()ï¼šè¿”å›ç¬¦åˆåŒ¹é…æ¡ä»¶çš„æ‰€æœ‰æ–‡ä»¶çš„è·¯å¾„ï¼›
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]

# åˆå§‹åŒ–strä¸ºè·¯å¾„ å¦åˆ™æŠ¥é”™ï¼šAttributeError: 'str' object has no attribute 'glob'
path = Path(path)

train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

# å°†æµ‹è¯•é›†æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# TextLineDatasetè¯»å–csvæ‹†åˆ†ä¸ºnegå’Œposæ ‡ç­¾
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)


batch_size = 32
# å¯¹äºimdb_datasetæ“ä½œéœ€è¦cacheï¼ŒTextLineDatasetæ‰èƒ½è·å–å’Œä¹‹å‰openå‡½æ•°ä¸€æ ·çš„æ€§èƒ½
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# è¾“å…¥å‰è¦æˆªå–æˆç›¸åŒç¨‹åº¦çš„æ–‡æœ¬padçš„ä½œç”¨ä¹Ÿæ˜¯ä¿æŒæ¯ä¸ªtokençš„å•ä½é•¿åº¦
def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)


# ç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­tokençš„å‡ºç°æ¬¡æ•°å‡½æ•°
def get_vocabulary(data_sample, max_size=1000):
    preprocessed_reviews = preprocess(data_sample).numpy()
    counter = Counter()
    for words in preprocessed_reviews:
        for word in words:
            if word != b&quot;&lt;pad&gt;&quot;:
                counter[word] += 1
    return [b&quot;&lt;pad&gt;&quot;] + [word for word, count in counter.most_common(max_size)]


get_vocabulary(X_example)


# 1ã€preprocess ç”Ÿæˆ token
# 2ã€get_vocabulary ç»Ÿè®¡é¢‘ç‡
# å°†tokenç»Ÿè®¡é¢‘ç‡åçš„å½¢æˆtableä¸indxæ¥æ”¶æ•´æ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨å†…éƒ¨å­—å…¸ä¸­æŸ¥æ‰¾è¿™äº›æ•´æ•°ï¼Œç„¶åè¿”å›ç›¸å…³è”çš„å‘é‡
class TextVectorization(keras.layers.Layer):
    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.max_vocabulary_size = max_vocabulary_size
        self.n_oov_buckets = n_oov_buckets

    def adapt(self, data_sample):
        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)
        words = tf.constant(self.vocab)
        word_ids = tf.range(len(self.vocab), dtype=tf.int64)
        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)

    def call(self, inputs):
        preprocessed_inputs = preprocess(inputs)
        return self.table.lookup(preprocessed_inputs)


text_vectorization = TextVectorization()

text_vectorization.adapt(X_example)
text_vectorization(X_example)

max_vocabulary_size = 1000
n_oov_buckets = 100

sample_review_batches = train_set.map(lambda review, label: review)
sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),
                                axis=0)

text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,
                                       input_shape=[])
text_vectorization.adapt(sample_reviews)
text_vectorization(X_example)

simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])
tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)

print('text_vectorization.vocab: ', text_vectorization.vocab[:10])


class BagOfWords(keras.layers.Layer):
    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):
        super().__init__(dtype=dtype, **kwargs)
        self.n_tokens = n_tokens

    def call(self, inputs):
        one_hot = tf.one_hot(inputs, self.n_tokens)
        return tf.reduce_sum(one_hot, axis=1)[:, 1:]


bag_of_words = BagOfWords(n_tokens=4)
print('bag_of_words(simple_example): ', bag_of_words(simple_example))

n_tokens = max_vocabulary_size + n_oov_buckets + 1  # add 1 for &lt;pad&gt;
bag_of_words = BagOfWords(n_tokens)

model = keras.models.Sequential([
    text_vectorization,
    bag_of_words,
    keras.layers.Dense(100, activation=&quot;relu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;),
])
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])
model.fit(train_set, epochs=5, validation_data=valid_set)


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP_00001]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_practice_00001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_practice_00001/">
        </link>
        <updated>2021-05-08T01:31:11.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>Taskï¼šå¯¹æ•°æ®é›†è¿›è¡Œæ‹†åˆ†ï¼Œåˆ›å»ºä¸€ä¸ªtf.data.Datasetæ¥åŠ è½½å¹¶è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶ååˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªåŒ…å«Embeddingå±‚çš„äºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹</p>
</blockquote>
<p>1.ä¸‹è½½å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†ï¼›</p>
<p>2.å°†æµ‹è¯•é›†æ‹†åˆ†ä¸ºä¸€ä¸ªéªŒè¯é›†å’Œä¸€ä¸ªæµ‹è¯•é›†</p>
<p>3.åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶åˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨TextVectorizationå±‚å¯¹æ¯ä¸ªè¯„è®ºè¿›è¡Œé¢„å¤„ç†ã€‚å¦‚æœTextVectorizationå±‚å°šä¸å¯ç”¨ï¼Œè¯·è‡ªå®šä¹‰é¢„å¤„ç†å±‚ã€‚</p>
<pre><code># TensorFlow â‰¥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

from IPython import get_ipython

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(7)

# åŠ è½½æ•°æ®é›†
input_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/raw/'
output_dir = 'C:/Users/ericf/PycharmProjects/data_scientenist/data/processed/'
# ## 10.
# _Exercise: In this exercise you will download a dataset, split it,
# create a `tf.data.Dataset` to load it and preprocess it efficiently,
# then build and train a binary classification model containing an `Embedding` layer._
#
# ### a.
# _Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb),
# which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/).
# The data is organized in two directories, `train` and `test`,
# each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews.
# Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words),
# but we will ignore them in this exercise._




from pathlib import Path

# ä½¿ç”¨keras.utils.get_fileä¸‹è½½imdbæ•°æ®é›†ï¼Œcache_diræŒ‡å®šè‡ªå·±çš„æ–‡ä»¶ç›®å½•ï¼›
DOWNLOAD_ROOT = &quot;http://ai.stanford.edu/~amaas/data/sentiment/&quot;
FILENAME = &quot;aclImdb_v1.tar.gz&quot;
filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True,cache_dir=input_dir)
path = Path(filepath).parent / &quot;aclImdb&quot;
print(path)
# C:\Users\ericf\PycharmProjects\data_scientenist\data\raw\datasets\aclImdb

# æ‰“å°æ•°æ®é›†çš„å±‚çº§å…³ç³»å’Œæ–‡ä»¶å
#  os.walkæ ¹ç›®å½•ä¸‹çš„æ¯ä¸€ä¸ªæ–‡ä»¶å¤¹(åŒ…å«å®ƒè‡ªå·±), äº§ç”Ÿ3-å…ƒç»„ (dirpath, dirnames, filenames)ã€æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶å¤¹åå­—, æ–‡ä»¶åã€‘
#  name, subdirs, files=ã€æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶å¤¹åå­—, æ–‡ä»¶åã€‘
#  indent æ ¹æ®æ–‡ä»¶å¤¹ç›®å½•è®°å½•ç¼©è¿›
for name, subdirs, files in os.walk(path):
    indent = len(Path(name).parts) - len(path.parts)
    print(&quot;    &quot; * indent + Path(name).parts[-1] + os.sep)
    for index, filename in enumerate(sorted(files)):
        if index == 3:
            print(&quot;    &quot; * (indent + 1) + &quot;...&quot;)
            break
        print(&quot;    &quot; * (indent + 1) + filename)

#glob.glob()ï¼šè¿”å›ç¬¦åˆåŒ¹é…æ¡ä»¶çš„æ‰€æœ‰æ–‡ä»¶çš„è·¯å¾„ï¼›
def review_paths(dirpath):
    return [str(path) for path in dirpath.glob(&quot;*.txt&quot;)]


train_pos = review_paths(path / &quot;train&quot; / &quot;pos&quot;)
train_neg = review_paths(path / &quot;train&quot; / &quot;neg&quot;)
test_valid_pos = review_paths(path / &quot;test&quot; / &quot;pos&quot;)
test_valid_neg = review_paths(path / &quot;test&quot; / &quot;neg&quot;)

print(len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg))

# ### b.
# _Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._
# å°†æµ‹è¯•é›†æ‹†åˆ†æµ‹è¯•é›†å’ŒéªŒè¯é›†
np.random.shuffle(test_valid_pos)

test_pos = test_valid_pos[:5000]
test_neg = test_valid_neg[:5000]
valid_pos = test_valid_pos[5000:]
valid_neg = test_valid_neg[5000:]


# ### c.
# _Exercise: Use tf.data to create an efficient dataset for each set._

# Since the dataset fits in memory,
# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:

# å¯¹äºå½±è¯„è¿›è¡ŒäºŒåˆ†ç±»æ ‡ç­¾å¤„ç†
def imdb_dataset(filepaths_positive, filepaths_negative):
    reviews = []
    labels = []
    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):
        for filepath in filepaths:
            with open(filepath,encoding='utf-8') as review_file:
                reviews.append(review_file.read())
            labels.append(label)
    return tf.data.Dataset.from_tensor_slices(
        (tf.constant(reviews), tf.constant(labels)))



for X, y in imdb_dataset(train_pos, train_neg).take(3):
    print(X)
    print(y)
    print()

#get_ipython().run_line_magic('timeit', '-r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass')


# It takes about 17 seconds to load the dataset and go through it 10 times.

# But let's pretend the dataset does not fit in memory, just to make things more interesting.
# Luckily, each review fits on just one line (they use `&lt;br /&gt;` to indicate line breaks),
# so we can read the reviews using a `TextLineDataset`.
# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).
# For very large datasets, it would make sense to use a tool like Apache Beam for that.


# 1.tf.data.TextLineDataset è¯»å–æ•°æ®é›†ï¼Œè‡ªåŠ¨åˆ›å»ºdataset
# 2.è¯»å–ä¹‹åä½¿ç”¨mapæ ¹æ®poså’Œnegè¿›è¡Œæ‰“æ ‡ç­¾æ“ä½œ
def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):
    dataset_neg = tf.data.TextLineDataset(filepaths_negative,
                                          num_parallel_reads=n_read_threads)
    dataset_neg = dataset_neg.map(lambda review: (review, 0))
    dataset_pos = tf.data.TextLineDataset(filepaths_positive,
                                          num_parallel_reads=n_read_threads)
    dataset_pos = dataset_pos.map(lambda review: (review, 1))
    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)




# Now it takes about 33 seconds to go through the dataset 10 times. That's much slower,
# essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.
# If you add `.cache()` just before `.repeat(10)`,
# you will see that this implementation will be about as fast as the previous one.



batch_size = 32
# å¯¹äºimdb_datasetæ“ä½œéœ€è¦cacheï¼ŒTextLineDatasetæ‰èƒ½è·å–å’Œä¹‹å‰openå‡½æ•°ä¸€æ ·çš„æ€§èƒ½
train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)
valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)
test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)


# ### d.
# _Exercise: Create a binary classification model,
# using a `TextVectorization` layer to preprocess each review.
# If the `TextVectorization` layer is not yet available (or if you like a challenge),
# try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package,
# for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces,
# and `split()` to split words on spaces.
# You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._

# Let's first write a function to preprocess the reviews, cropping them to 300 characters,
# converting them to lower case, then replacing `&lt;br /&gt;` and all non-letter characters to spaces,
# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:




def preprocess(X_batch, n_words=50):
    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])
    Z = tf.strings.substr(X_batch, 0, 300)
    Z = tf.strings.lower(Z)
    Z = tf.strings.regex_replace(Z, b&quot;&lt;br\\s*/?&gt;&quot;, b&quot; &quot;)
    Z = tf.strings.regex_replace(Z, b&quot;[^a-z]&quot;, b&quot; &quot;)
    Z = tf.strings.split(Z)
    return Z.to_tensor(shape=shape, default_value=b&quot;&lt;pad&gt;&quot;)


X_example = tf.constant([&quot;It's a great, great movie! I loved it.&quot;, &quot;It was terrible, run away!!!&quot;])
preprocess(X_example)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSK_999]]></title>
        <id>https://fa7ria.GitHub.io/datascientist_-xue-_001/</id>
        <link href="https://fa7ria.GitHub.io/datascientist_-xue-_001/">
        </link>
        <updated>2021-05-07T08:33:11.000Z</updated>
        <content type="html"><![CDATA[<p>#Adaboostã€GBDTã€XGBoostçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<p>Gradient Boosting å’Œå…¶å®ƒ Boosting ç®—æ³•ä¸€æ ·ï¼Œé€šè¿‡å°†è¡¨ç°ä¸€èˆ¬çš„æ•°ä¸ªæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯æ·±åº¦å›ºå®šçš„å†³ç­–æ ‘ï¼‰ç»„åˆåœ¨ä¸€èµ·æ¥é›†æˆä¸€ä¸ªè¡¨ç°è¾ƒå¥½çš„æ¨¡å‹ã€‚æŠ½è±¡åœ°è¯´ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ˜¯å¯¹ä¸€ä»»æ„å¯å¯¼ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡åå¤åœ°é€‰æ‹©ä¸€ä¸ªæŒ‡å‘è´Ÿæ¢¯åº¦æ–¹å‘çš„å‡½æ•°ï¼Œè¯¥ç®—æ³•å¯è¢«çœ‹åšåœ¨å‡½æ•°ç©ºé—´é‡Œå¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤å¯ä»¥è¯´ Gradient Boosting = Gradient Descent + Boostingã€‚</p>
<p>å’Œ AdaBoost ä¸€æ ·ï¼ŒGradient Boosting ä¹Ÿæ˜¯é‡å¤é€‰æ‹©ä¸€ä¸ªè¡¨ç°ä¸€èˆ¬çš„æ¨¡å‹å¹¶ä¸”æ¯æ¬¡åŸºäºå…ˆå‰æ¨¡å‹çš„è¡¨ç°è¿›è¡Œè°ƒæ•´ã€‚ä¸åŒçš„æ˜¯ï¼ŒAdaBoost æ˜¯é€šè¿‡æå‡é”™åˆ†æ•°æ®ç‚¹çš„æƒé‡æ¥å®šä½æ¨¡å‹çš„ä¸è¶³è€Œ Gradient Boosting æ˜¯é€šè¿‡ç®—æ¢¯åº¦ï¼ˆgradientï¼‰æ¥å®šä½æ¨¡å‹çš„ä¸è¶³ã€‚å› æ­¤ç›¸æ¯” AdaBoost, Gradient Boosting å¯ä»¥ä½¿ç”¨æ›´å¤šç§ç±»çš„ç›®æ ‡å‡½æ•°ã€‚</p>
<p><strong>Adaboost VS GBDTæœ€ä¸»è¦çš„åŒºåˆ«åœ¨äºä¸¤è€…å¦‚ä½•è¯†åˆ«æ¨¡å‹çš„é—®é¢˜</strong>ã€‚AdaBoostç”¨é”™åˆ†æ•°æ®ç‚¹æ¥è¯†åˆ«é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´é”™åˆ†æ•°æ®ç‚¹çš„æƒé‡æ¥æ”¹è¿›æ¨¡å‹ã€‚Gradient Boostingé€šè¿‡è´Ÿæ¢¯åº¦æ¥è¯†åˆ«é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—è´Ÿæ¢¯åº¦æ¥æ”¹è¿›æ¨¡å‹ã€‚</p>
<p>#GBDT ä¸ XGBoost åŒºåˆ«<br>
ä¼ ç»ŸGBDTä»¥CARTä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œxgboostè¿˜æ”¯æŒçº¿æ€§åˆ†ç±»å™¨ï¼Œè¿™ä¸ªæ—¶å€™xgboostç›¸å½“äºå¸¦L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„é€»è¾‘æ–¯è’‚å›å½’ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰æˆ–è€…çº¿æ€§å›å½’ï¼ˆå›å½’é—®é¢˜ï¼‰ã€‚<br>
ä¼ ç»ŸGBDTåœ¨ä¼˜åŒ–æ—¶åªç”¨åˆ°ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼Œxgbooståˆ™å¯¹ä»£ä»·å‡½æ•°è¿›è¡Œäº†äºŒé˜¶æ³°å‹’å±•å¼€ï¼ŒåŒæ—¶ç”¨åˆ°äº†ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ã€‚</p>
<p>xgbooståœ¨ä»£ä»·å‡½æ•°é‡ŒåŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€æ¯ä¸ªå¶å­èŠ‚ç‚¹ä¸Šè¾“å‡ºçš„scoreçš„L2æ¨¡çš„å¹³æ–¹å’Œã€‚ä»Bias-variance tradeoffè§’åº¦æ¥è®²ï¼Œæ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„varianceï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè¿™ä¹Ÿæ˜¯xgboostä¼˜äºä¼ ç»ŸGBDTçš„ä¸€ä¸ªç‰¹æ€§ã€‚</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://fa7ria.GitHub.io/hello-gridea/</id>
        <link href="https://fa7ria.GitHub.io/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>